{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "# Importing TF and checking the version\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing helper functions\n",
    "from DanielBourke_HelperFunctions import create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"NLP_text/train.csv\")\n",
    "test_df = pd.read_csv(\"NLP_text/test.csv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffling training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking test dataframe\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking number of training records\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking total number of samples\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "#Metepec #Mexico - ?NIGHT DISASTER?...E(Oficial) @ #NitClub #mÌ¼sica #mÌ¼sica http://t.co/WTfJF9jjzs\n",
      "---\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "@alextucker VOLCANO BOWL DRINK\n",
      "---\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "The 8-Minute Fat-Burning Routine ThatÛªs Also Really Fun http://t.co/g2h7xNecD8 #fat weightless # fatburning #burnfat #skinny #workout\n",
      "---\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Factors to Consider When Hiring a Demolition Company nOxDV\n",
      "---\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "#TBT Remember that time Patrick Kane attacked a cab driver over .20\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Visualising random samples\n",
    "import random\n",
    "\n",
    "random_index = random.randint(0, len(train_df) - 5)\n",
    "\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index : random_index + 5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    train_df_shuffled[\"text\"].to_numpy(),\n",
    "    train_df_shuffled[\"target\"].to_numpy(),\n",
    "    test_size=0.1, # Allocating 10% to validation data\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking dataset length\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the first 10 samples\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using text vectorisation\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=None,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    ngrams=None,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    "    pad_to_max_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the average number of tokens\n",
    "round(sum([len(i.split()) for i in train_sentences]) / len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up text vectorisation variables\n",
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting text vectorisation to the training dataset\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 74,   9,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample sentnence and tokenise it\n",
    "sample_sentence = \"There is a flood in my street\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "@RosemaryTravale Do we all use the same weapon? 'cause we might be screwed XD,\n",
      "tokenised version:\n",
      "[[8978   68   46   44  447    2  726  263  257   46  843   21 8730 1916\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenising random sentence from the training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(\n",
    "    f\"Original text:\\n{random_sentence},\\ntokenised version:\\n{text_vectorizer([random_sentence])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " ['', '[UNK]', 'the', 'a', 'in'],\n",
       " ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "\n",
    "len(words_in_vocab), top_5_words, bottom_5_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x242a39b2520>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the layer\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length\n",
    ")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "@CouncilSCC it does say hailstorm,\n",
      "embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.01347689,  0.01081227, -0.01314595, ...,  0.01134624,\n",
       "         -0.03508957,  0.01095806],\n",
       "        [-0.04150762, -0.02875059, -0.02448224, ..., -0.02722785,\n",
       "          0.01106387,  0.03568279],\n",
       "        [-0.00855424, -0.0311479 , -0.00529737, ...,  0.01986209,\n",
       "         -0.04528741,  0.02708408],\n",
       "        ...,\n",
       "        [ 0.03131826, -0.01514022,  0.02296391, ...,  0.0005614 ,\n",
       "          0.03330268, -0.02141659],\n",
       "        [ 0.03131826, -0.01514022,  0.02296391, ...,  0.0005614 ,\n",
       "          0.03330268, -0.02141659],\n",
       "        [ 0.03131826, -0.01514022,  0.02296391, ...,  0.0005614 ,\n",
       "          0.03330268, -0.02141659]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(\n",
    "    f\"Original text:\\n{random_sentence},\\nembedded version:\"\n",
    ")\n",
    "\n",
    "# Embed random sentence\n",
    "embed_sentence = embedding(text_vectorizer([random_sentence]))\n",
    "embed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.01347689,  0.01081227, -0.01314595,  0.02223697, -0.02041599,\n",
       "        -0.00706041,  0.0452088 , -0.03344657, -0.04105613,  0.00650709,\n",
       "         0.0163715 , -0.02625236, -0.0369825 ,  0.00570083, -0.00296791,\n",
       "        -0.0094461 ,  0.00617895, -0.01487704, -0.03923587, -0.01148658,\n",
       "        -0.01263583, -0.01754203, -0.01623712,  0.00919318, -0.02260126,\n",
       "        -0.03906111, -0.02007184, -0.03806884, -0.02933215,  0.00250695,\n",
       "         0.00178555,  0.03941614, -0.01576471, -0.02000595, -0.00528374,\n",
       "        -0.02230659, -0.00863902,  0.00590007, -0.04030498, -0.01158222,\n",
       "        -0.0450322 , -0.0207773 , -0.01431553,  0.03859941, -0.03628879,\n",
       "         0.03357115,  0.04113933,  0.04809996,  0.03796258, -0.0056957 ,\n",
       "         0.02898171,  0.00693237, -0.02856143,  0.04935229, -0.02329591,\n",
       "         0.04564562, -0.0317139 , -0.04288054,  0.03877959,  0.03154181,\n",
       "         0.02652296,  0.00544713,  0.03085067, -0.03547785,  0.01919732,\n",
       "        -0.03195536, -0.00438   , -0.02314264,  0.0197984 ,  0.04134784,\n",
       "        -0.01119124,  0.04188685, -0.02195698,  0.01984862, -0.00267721,\n",
       "         0.01873398, -0.02134525,  0.00520915,  0.017308  , -0.02255582,\n",
       "        -0.01683896,  0.0193512 ,  0.04047828,  0.01776595,  0.0081404 ,\n",
       "         0.00512333, -0.01085899, -0.01749021, -0.04711915, -0.01310338,\n",
       "         0.0287425 , -0.02065995, -0.04528285, -0.03778422,  0.04590591,\n",
       "        -0.0149766 ,  0.00973322, -0.02223438, -0.02048855, -0.00740438,\n",
       "         0.03016278, -0.04352474,  0.02670649,  0.04549843, -0.04448096,\n",
       "         0.01594985,  0.04963722,  0.01609877,  0.04086283,  0.04831386,\n",
       "        -0.01795457,  0.03830966, -0.04327039,  0.0161563 ,  0.01667735,\n",
       "        -0.02142168, -0.02344381,  0.03431055,  0.01324815, -0.01801967,\n",
       "         0.02188521, -0.00898058,  0.02112338,  0.00254513, -0.02679857,\n",
       "         0.01134624, -0.03508957,  0.01095806], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " '@CouncilSCC it does say hailstorm')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking single token's embedding\n",
    "embed_sentence[0][0], embed_sentence[0][0].shape, random_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using SKLearn to build base model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenisation and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # Convert words to numbers\n",
    "    (\"clf\", MultinomialNB()) # Model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved accuracy: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating baseline model\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Achieved accuracy: {baseline_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "baseline_predictions = model_0.predict(val_sentences)\n",
    "baseline_predictions[:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing SKLearn functions\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Developing function to evaluate accuracy, precision, recall and F1 scor\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate binary classification model\n",
    "    \"\"\"\n",
    "    # Calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    # Calculate model precision, recall and F1 score using \"weighted\" average\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\n",
    "        \"accuracy\": model_accuracy,\n",
    "        \"precision\": model_precision,\n",
    "        \"recall\": model_recall,\n",
    "        \"f1\": model_f1\n",
    "    }\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting baseline results\n",
    "baseline_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=baseline_predictions\n",
    ")\n",
    "baseline_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: a simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensorboard callback\n",
    "from DanielBourke_HelperFunctions import create_tensorboard_callback\n",
    "\n",
    "# Creating log directory\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with Functional API\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype=tf.string)\n",
    "x = text_vectorizer(inputs) # Turn the input text into numbers\n",
    "x = embedding(x) # Create embedding of numberised inputs\n",
    "x = layers.GlobalAveragePooling1D()(x) # Condence the feature vector\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Getting model summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model\n",
    "model_1.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_1_dense/20230114-161732\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 9s 32ms/step - loss: 0.6130 - accuracy: 0.6895 - val_loss: 0.5352 - val_accuracy: 0.7612\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.4412 - accuracy: 0.8210 - val_loss: 0.4749 - val_accuracy: 0.7822\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.3476 - accuracy: 0.8605 - val_loss: 0.4555 - val_accuracy: 0.7913\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.2852 - accuracy: 0.8908 - val_loss: 0.4640 - val_accuracy: 0.7848\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.2380 - accuracy: 0.9124 - val_loss: 0.4751 - val_accuracy: 0.7756\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "history_1 = model_1.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_1_dense\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 0.4751 - accuracy: 0.7756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47509005665779114, 0.7755905389785767]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the results\n",
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(762, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4217687 ],\n",
       "       [0.73004496],\n",
       "       [0.9975582 ],\n",
       "       [0.16141163],\n",
       "       [0.12978622],\n",
       "       [0.94919074],\n",
       "       [0.9181515 ],\n",
       "       [0.9933853 ],\n",
       "       [0.96926194],\n",
       "       [0.33147788]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 predictions\n",
    "model_1_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting model predictions to label format\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.55905511811024,\n",
       " 'precision': 0.7772070861555818,\n",
       " 'recall': 0.7755905511811023,\n",
       " 'f1': 0.7736182129212565}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating model 1 results\n",
    "model_1_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_1_preds\n",
    ")\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving baseline results\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing model results\n",
    "import numpy as np\n",
    "\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting vocabulary from the text vectorisation layer\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Checking model 1 details\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.015783  , -0.00334908,  0.03756675, ...,  0.01565457,\n",
       "           0.01792214, -0.03679121],\n",
       "         [-0.02315077,  0.01504393, -0.00547965, ...,  0.01842208,\n",
       "          -0.04491741,  0.0018163 ],\n",
       "         [-0.0224451 ,  0.00997871, -0.00996   , ...,  0.00733669,\n",
       "          -0.04458904, -0.00248135],\n",
       "         ...,\n",
       "         [ 0.03404609,  0.0221143 ,  0.02656981, ..., -0.04279038,\n",
       "          -0.03772571, -0.00165086],\n",
       "         [-0.02283798,  0.03905335,  0.0754346 , ...,  0.03326756,\n",
       "          -0.02747221, -0.04922141],\n",
       "         [-0.05063306,  0.05983922,  0.03132253, ...,  0.03882468,\n",
       "          -0.08314517, -0.04567678]], dtype=float32)],\n",
       " (10000, 128))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the weights\n",
    "embed_weights = model_1.get_layer(\"embedding\").get_weights()\n",
    "embed_weights, embed_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embedding files (sample from TF Word Embedding documentation)\n",
    "import io\n",
    "\n",
    "out_v = io.open(\"vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for index, word in enumerate(words_in_vocab):\n",
    "    if index == 0:\n",
    "        continue # Skip 0, as it's a padding\n",
    "    vec = embed_weights[0][index]\n",
    "    out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated vectors.tsv and metadata.tsv can be uploaded into Tensorflow Embedding Projector tool at https://projector.tensorflow.org/ to visualise weights in 3D space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - RNN (Recurrent Neural Network) with LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15, 128)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "# CReating an RNN model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "print(x.shape)\n",
    "# x = layers.LSTM(units=64, return_sequences=True)(x) # Sequences required when you stack LSTM layers\n",
    "# print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "print(x.shape)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Checking model's summary\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_2.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_2_LSTM/20230114-161800\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 11s 29ms/step - loss: 0.2251 - accuracy: 0.9225 - val_loss: 0.6094 - val_accuracy: 0.7743\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 6s 26ms/step - loss: 0.1574 - accuracy: 0.9402 - val_loss: 0.6687 - val_accuracy: 0.7874\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 6s 26ms/step - loss: 0.1277 - accuracy: 0.9531 - val_loss: 0.7074 - val_accuracy: 0.7756\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1060 - accuracy: 0.9607 - val_loss: 0.8200 - val_accuracy: 0.7822\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.0864 - accuracy: 0.9667 - val_loss: 0.8727 - val_accuracy: 0.7808\n"
     ]
    }
   ],
   "source": [
    "# Fittting the model\n",
    "history_2 = model_2.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_2_LSTM\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03102198],\n",
       "       [0.9560031 ],\n",
       "       [0.999782  ],\n",
       "       [0.09847017],\n",
       "       [0.00103051],\n",
       "       [0.9991949 ],\n",
       "       [0.8240903 ],\n",
       "       [0.999859  ],\n",
       "       [0.99975246],\n",
       "       [0.4020007 ]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meking predictions\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting pred probs to label format\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.08398950131233,\n",
       " 'precision': 0.7816731653312017,\n",
       " 'recall': 0.7808398950131233,\n",
       " 'f1': 0.7793746882098481}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate model 2 results\n",
    "model_2_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_2_preds\n",
    ")\n",
    "model_2_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - RNN with GRU (gated recurrent unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "# x = layers.GRU(64, return_sequences=True)(x) # Sequences, if GRU are stuck on each other\n",
    "x = layers.GRU(64)(x)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x)\n",
    "# x = layers.GlobalAveragePooling1D()(x) # If we want to consolidate GRU layer with sequences\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Getting model summary\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_3.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_3_GRU/20230114-161833\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 8s 23ms/step - loss: 0.1577 - accuracy: 0.9390 - val_loss: 0.7244 - val_accuracy: 0.7756\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.0817 - accuracy: 0.9680 - val_loss: 0.7984 - val_accuracy: 0.7730\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.0717 - accuracy: 0.9737 - val_loss: 0.9451 - val_accuracy: 0.7782\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.0576 - accuracy: 0.9758 - val_loss: 1.1389 - val_accuracy: 0.7782\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 20ms/step - loss: 0.0499 - accuracy: 0.9780 - val_loss: 1.4163 - val_accuracy: 0.7651\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "history_3 = model_3.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_3_GRU\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.08835559e-04],\n",
       "       [7.95614719e-01],\n",
       "       [9.99934494e-01],\n",
       "       [1.38800200e-02],\n",
       "       [8.55540420e-05],\n",
       "       [9.99913275e-01],\n",
       "       [9.91664350e-01],\n",
       "       [9.99975920e-01],\n",
       "       [9.99946177e-01],\n",
       "       [9.69961941e-01],\n",
       "       [1.96265537e-04],\n",
       "       [9.95267332e-01],\n",
       "       [1.75115973e-04],\n",
       "       [2.09964991e-01],\n",
       "       [9.34759591e-05],\n",
       "       [5.71150158e-04],\n",
       "       [2.42611219e-04],\n",
       "       [2.58447835e-04],\n",
       "       [2.17885850e-03],\n",
       "       [9.99916077e-01],\n",
       "       [9.99982059e-01],\n",
       "       [4.79268674e-05],\n",
       "       [9.99823987e-01],\n",
       "       [5.38544846e-04],\n",
       "       [9.99949515e-01],\n",
       "       [9.99961197e-01],\n",
       "       [4.15697577e-04],\n",
       "       [6.78526005e-04],\n",
       "       [1.28086089e-04],\n",
       "       [5.46225369e-01],\n",
       "       [9.99759257e-01],\n",
       "       [1.99859249e-04],\n",
       "       [2.69128545e-03],\n",
       "       [6.82729646e-04],\n",
       "       [9.97484922e-01],\n",
       "       [1.45861760e-01],\n",
       "       [9.99923706e-01],\n",
       "       [6.11586608e-02],\n",
       "       [7.71175465e-03],\n",
       "       [9.99959648e-01],\n",
       "       [6.96934700e-01],\n",
       "       [5.82740904e-05],\n",
       "       [9.89625514e-01],\n",
       "       [1.13373870e-04],\n",
       "       [9.68785644e-01],\n",
       "       [9.99928355e-01],\n",
       "       [9.98729527e-01],\n",
       "       [9.63497221e-01],\n",
       "       [4.34214307e-04],\n",
       "       [7.64300287e-01],\n",
       "       [1.63303863e-03],\n",
       "       [4.13116753e-01],\n",
       "       [1.13655012e-02],\n",
       "       [1.15052529e-03],\n",
       "       [8.17540884e-01],\n",
       "       [5.65459207e-03],\n",
       "       [7.67893274e-04],\n",
       "       [9.99954760e-01],\n",
       "       [1.48112624e-04],\n",
       "       [5.04391501e-04],\n",
       "       [7.86015019e-03],\n",
       "       [9.99941707e-01],\n",
       "       [9.99428034e-01],\n",
       "       [4.21041250e-03],\n",
       "       [9.99977946e-01],\n",
       "       [9.99970675e-01],\n",
       "       [9.95362639e-01],\n",
       "       [9.95706022e-01],\n",
       "       [9.99405146e-01],\n",
       "       [6.04996178e-03],\n",
       "       [4.66564894e-02],\n",
       "       [3.50477695e-02],\n",
       "       [9.99957502e-01],\n",
       "       [2.49468605e-04],\n",
       "       [5.89048900e-02],\n",
       "       [9.99632478e-01],\n",
       "       [4.98642214e-03],\n",
       "       [9.99849737e-01],\n",
       "       [1.71212368e-02],\n",
       "       [1.97442938e-02],\n",
       "       [2.04685814e-04],\n",
       "       [9.88051966e-02],\n",
       "       [9.99955654e-01],\n",
       "       [4.08111431e-04],\n",
       "       [2.95914506e-04],\n",
       "       [4.96683642e-04],\n",
       "       [1.86433637e-04],\n",
       "       [2.87054630e-04],\n",
       "       [2.26325370e-04],\n",
       "       [9.99930024e-01],\n",
       "       [9.99933004e-01],\n",
       "       [1.30505330e-04],\n",
       "       [9.94194865e-01],\n",
       "       [1.11636153e-04],\n",
       "       [9.99949276e-01],\n",
       "       [7.34113693e-01],\n",
       "       [9.68697429e-01],\n",
       "       [9.99979436e-01],\n",
       "       [9.99893606e-01],\n",
       "       [9.94511664e-01],\n",
       "       [9.99982297e-01],\n",
       "       [8.95021018e-04],\n",
       "       [6.88484943e-05],\n",
       "       [9.99685466e-01],\n",
       "       [9.99919653e-01],\n",
       "       [2.95331329e-03],\n",
       "       [9.99910712e-01],\n",
       "       [9.99419808e-01],\n",
       "       [3.39305843e-04],\n",
       "       [9.99902606e-01],\n",
       "       [9.99528468e-01],\n",
       "       [2.91256467e-04],\n",
       "       [1.68121800e-01],\n",
       "       [3.82501894e-04],\n",
       "       [1.66575395e-04],\n",
       "       [7.05810338e-02],\n",
       "       [4.19665217e-01],\n",
       "       [9.99898732e-01],\n",
       "       [5.30537101e-04],\n",
       "       [1.59043499e-04],\n",
       "       [9.99962509e-01],\n",
       "       [9.15168566e-05],\n",
       "       [7.99795598e-05],\n",
       "       [9.98182952e-01],\n",
       "       [1.74945977e-03],\n",
       "       [1.95239729e-04],\n",
       "       [9.99888182e-01],\n",
       "       [8.56838597e-05],\n",
       "       [4.37923882e-04],\n",
       "       [9.99849260e-01],\n",
       "       [4.28276428e-04],\n",
       "       [9.99962509e-01],\n",
       "       [9.99970436e-01],\n",
       "       [9.99961197e-01],\n",
       "       [9.99924898e-01],\n",
       "       [4.08458203e-04],\n",
       "       [9.99936521e-01],\n",
       "       [4.12268797e-03],\n",
       "       [1.47321052e-03],\n",
       "       [1.92829626e-04],\n",
       "       [9.99970913e-01],\n",
       "       [9.58728731e-01],\n",
       "       [2.09964991e-01],\n",
       "       [9.99855995e-01],\n",
       "       [3.74686755e-02],\n",
       "       [6.87150809e-04],\n",
       "       [1.56043505e-04],\n",
       "       [7.78956237e-05],\n",
       "       [9.24987078e-01],\n",
       "       [9.99926627e-01],\n",
       "       [1.81851938e-04],\n",
       "       [5.66778192e-03],\n",
       "       [3.16785241e-04],\n",
       "       [1.81507712e-04],\n",
       "       [3.32827243e-04],\n",
       "       [9.99904037e-01],\n",
       "       [9.99196410e-01],\n",
       "       [4.51340544e-04],\n",
       "       [9.99881566e-01],\n",
       "       [1.22058496e-04],\n",
       "       [9.99957681e-01],\n",
       "       [2.52428139e-03],\n",
       "       [5.64112961e-02],\n",
       "       [9.99624133e-01],\n",
       "       [1.56565774e-02],\n",
       "       [6.14467644e-05],\n",
       "       [9.99950111e-01],\n",
       "       [2.64031929e-03],\n",
       "       [9.99962687e-01],\n",
       "       [7.09171057e-01],\n",
       "       [9.99954879e-01],\n",
       "       [9.99776959e-01],\n",
       "       [9.99920547e-01],\n",
       "       [2.56466214e-04],\n",
       "       [9.99984503e-01],\n",
       "       [2.50197743e-04],\n",
       "       [8.84706213e-04],\n",
       "       [1.07592507e-03],\n",
       "       [3.23945552e-01],\n",
       "       [9.99942720e-01],\n",
       "       [1.04905781e-03],\n",
       "       [3.83699626e-01],\n",
       "       [9.99912977e-01],\n",
       "       [9.99952018e-01],\n",
       "       [9.99960423e-01],\n",
       "       [1.72567437e-04],\n",
       "       [1.63704928e-04],\n",
       "       [9.99981105e-01],\n",
       "       [6.22865628e-05],\n",
       "       [7.79647307e-05],\n",
       "       [4.88596922e-03],\n",
       "       [9.99902844e-01],\n",
       "       [2.26920456e-04],\n",
       "       [1.50490610e-04],\n",
       "       [1.35920563e-04],\n",
       "       [3.48322559e-04],\n",
       "       [2.05169214e-04],\n",
       "       [1.38582569e-03],\n",
       "       [9.99910355e-01],\n",
       "       [1.13803381e-03],\n",
       "       [4.69593564e-03],\n",
       "       [9.99944329e-01],\n",
       "       [9.99934971e-01],\n",
       "       [7.30739441e-04],\n",
       "       [1.97077330e-04],\n",
       "       [9.99936759e-01],\n",
       "       [9.99904990e-01],\n",
       "       [9.99956191e-01],\n",
       "       [9.97527599e-01],\n",
       "       [9.99333322e-01],\n",
       "       [2.03958414e-02],\n",
       "       [9.99940276e-01],\n",
       "       [1.21146171e-04],\n",
       "       [3.31449235e-04],\n",
       "       [1.35677896e-04],\n",
       "       [7.14921262e-05],\n",
       "       [9.99945879e-01],\n",
       "       [9.99182880e-01],\n",
       "       [9.99564886e-01],\n",
       "       [4.37414438e-01],\n",
       "       [9.60961044e-01],\n",
       "       [1.38303835e-03],\n",
       "       [5.99301979e-03],\n",
       "       [1.30890252e-03],\n",
       "       [9.99932885e-01],\n",
       "       [2.03008074e-02],\n",
       "       [6.34139054e-04],\n",
       "       [9.99980867e-01],\n",
       "       [9.99844849e-01],\n",
       "       [9.99338925e-01],\n",
       "       [1.14695911e-04],\n",
       "       [1.82974490e-03],\n",
       "       [9.99876022e-01],\n",
       "       [1.43900335e-01],\n",
       "       [1.00222476e-01],\n",
       "       [2.44213198e-03],\n",
       "       [2.17895508e-02],\n",
       "       [9.87086177e-01],\n",
       "       [4.41307173e-04],\n",
       "       [2.44555558e-04],\n",
       "       [9.99848843e-01],\n",
       "       [1.38283726e-02],\n",
       "       [9.99946117e-01],\n",
       "       [9.99923170e-01],\n",
       "       [2.04778145e-04],\n",
       "       [6.88748332e-05],\n",
       "       [9.99921322e-01],\n",
       "       [1.32437810e-04],\n",
       "       [1.11784542e-03],\n",
       "       [2.20557868e-01],\n",
       "       [1.78455331e-04],\n",
       "       [9.99849498e-01],\n",
       "       [7.16823924e-05],\n",
       "       [1.38349482e-03],\n",
       "       [9.99924719e-01],\n",
       "       [2.53021885e-02],\n",
       "       [9.99932468e-01],\n",
       "       [9.99969363e-01],\n",
       "       [8.91721323e-02],\n",
       "       [1.22386453e-04],\n",
       "       [1.75250426e-03],\n",
       "       [1.67038423e-04],\n",
       "       [4.00487188e-05],\n",
       "       [9.99896526e-01],\n",
       "       [9.99955773e-01],\n",
       "       [2.81349174e-03],\n",
       "       [9.99915898e-01],\n",
       "       [2.40002206e-04],\n",
       "       [6.29752583e-04],\n",
       "       [1.42081757e-04],\n",
       "       [1.84851873e-04],\n",
       "       [1.23086909e-04],\n",
       "       [9.99926627e-01],\n",
       "       [1.00097922e-03],\n",
       "       [1.23917271e-04],\n",
       "       [9.99915898e-01],\n",
       "       [1.48150473e-04],\n",
       "       [8.88961877e-05],\n",
       "       [9.99927282e-01],\n",
       "       [1.71265565e-04],\n",
       "       [8.92223045e-03],\n",
       "       [7.64070937e-05],\n",
       "       [9.99922574e-01],\n",
       "       [9.87444639e-01],\n",
       "       [5.69578588e-01],\n",
       "       [1.04478067e-02],\n",
       "       [9.99736845e-01],\n",
       "       [1.22800376e-03],\n",
       "       [9.96624112e-01],\n",
       "       [8.48587696e-03],\n",
       "       [9.98236120e-01],\n",
       "       [9.97864246e-01],\n",
       "       [9.99809861e-01],\n",
       "       [7.80885518e-01],\n",
       "       [2.78842496e-03],\n",
       "       [1.65539280e-01],\n",
       "       [1.07977819e-03],\n",
       "       [6.02305867e-04],\n",
       "       [2.71071884e-04],\n",
       "       [4.00085794e-03],\n",
       "       [4.92037361e-05],\n",
       "       [4.51105268e-04],\n",
       "       [6.13998063e-03],\n",
       "       [9.99946356e-01],\n",
       "       [2.14870524e-04],\n",
       "       [3.93369328e-03],\n",
       "       [3.03827263e-02],\n",
       "       [2.68956572e-02],\n",
       "       [3.58909041e-01],\n",
       "       [2.74886756e-04],\n",
       "       [1.13584327e-04],\n",
       "       [9.99910057e-01],\n",
       "       [1.60285532e-02],\n",
       "       [2.86820918e-01],\n",
       "       [9.99976695e-01],\n",
       "       [2.40247187e-04],\n",
       "       [9.46085930e-01],\n",
       "       [1.57903172e-02],\n",
       "       [2.38642353e-03],\n",
       "       [1.82258207e-02],\n",
       "       [1.79398776e-04],\n",
       "       [3.17623792e-03],\n",
       "       [9.99859691e-01],\n",
       "       [9.28458095e-01],\n",
       "       [9.99933839e-01],\n",
       "       [1.73726574e-01],\n",
       "       [1.59574702e-04],\n",
       "       [9.99944806e-01],\n",
       "       [2.73538288e-04],\n",
       "       [9.99942303e-01],\n",
       "       [1.32390996e-03],\n",
       "       [2.58137559e-04],\n",
       "       [9.99941885e-01],\n",
       "       [1.22054887e-04],\n",
       "       [2.41536356e-04],\n",
       "       [9.99904335e-01],\n",
       "       [1.51774191e-04],\n",
       "       [5.27564902e-04],\n",
       "       [7.31602252e-01],\n",
       "       [9.99249578e-01],\n",
       "       [1.21532386e-04],\n",
       "       [1.09765609e-03],\n",
       "       [9.99923825e-01],\n",
       "       [9.99974668e-01],\n",
       "       [9.99880791e-01],\n",
       "       [3.14449631e-02],\n",
       "       [9.99712646e-01],\n",
       "       [9.99618471e-01],\n",
       "       [6.34538708e-04],\n",
       "       [1.77956768e-04],\n",
       "       [2.57725641e-03],\n",
       "       [1.18208770e-02],\n",
       "       [1.15002993e-04],\n",
       "       [1.84301799e-03],\n",
       "       [1.84863314e-01],\n",
       "       [1.09369052e-04],\n",
       "       [9.99930859e-01],\n",
       "       [9.99961197e-01],\n",
       "       [9.99956667e-01],\n",
       "       [5.90952579e-04],\n",
       "       [1.96703672e-01],\n",
       "       [3.26292545e-01],\n",
       "       [4.13447842e-02],\n",
       "       [9.98799443e-01],\n",
       "       [3.81451696e-01],\n",
       "       [8.32953956e-05],\n",
       "       [1.71483232e-04],\n",
       "       [3.25486122e-04],\n",
       "       [9.99625742e-01],\n",
       "       [3.43568652e-04],\n",
       "       [1.71396846e-03],\n",
       "       [3.52561416e-04],\n",
       "       [2.29108162e-04],\n",
       "       [4.63818796e-02],\n",
       "       [9.97276068e-01],\n",
       "       [1.32930512e-02],\n",
       "       [2.06135504e-04],\n",
       "       [7.26144179e-04],\n",
       "       [1.47485058e-04],\n",
       "       [9.99952853e-01],\n",
       "       [9.99899268e-01],\n",
       "       [9.39226151e-01],\n",
       "       [9.98726189e-01],\n",
       "       [9.73807182e-05],\n",
       "       [9.98681009e-01],\n",
       "       [9.99952614e-01],\n",
       "       [8.71508598e-01],\n",
       "       [2.36360319e-02],\n",
       "       [9.99884248e-01],\n",
       "       [1.53428980e-03],\n",
       "       [9.99960363e-01],\n",
       "       [3.24196726e-01],\n",
       "       [1.07894780e-03],\n",
       "       [7.40159750e-02],\n",
       "       [7.84607053e-01],\n",
       "       [9.99939620e-01],\n",
       "       [2.51793545e-02],\n",
       "       [6.47298366e-05],\n",
       "       [1.83484299e-04],\n",
       "       [3.81451696e-01],\n",
       "       [9.99986053e-01],\n",
       "       [6.28788912e-05],\n",
       "       [9.04858708e-02],\n",
       "       [9.99977529e-01],\n",
       "       [3.76232747e-05],\n",
       "       [9.99962509e-01],\n",
       "       [6.56683813e-04],\n",
       "       [4.21077653e-04],\n",
       "       [1.44245292e-04],\n",
       "       [9.99900699e-01],\n",
       "       [9.99886692e-01],\n",
       "       [5.50559955e-04],\n",
       "       [2.58109241e-04],\n",
       "       [9.99914646e-01],\n",
       "       [9.99947011e-01],\n",
       "       [6.75360084e-01],\n",
       "       [1.37310373e-04],\n",
       "       [1.77452725e-03],\n",
       "       [9.36995149e-01],\n",
       "       [1.59103089e-04],\n",
       "       [9.99954760e-01],\n",
       "       [7.84140676e-02],\n",
       "       [9.99963701e-01],\n",
       "       [9.99936402e-01],\n",
       "       [5.29581262e-03],\n",
       "       [1.59042538e-03],\n",
       "       [1.14397856e-03],\n",
       "       [9.99891043e-01],\n",
       "       [4.48033690e-01],\n",
       "       [9.51787591e-01],\n",
       "       [6.81051053e-04],\n",
       "       [1.02065981e-03],\n",
       "       [4.25819599e-04],\n",
       "       [7.79399052e-05],\n",
       "       [2.06098915e-03],\n",
       "       [2.50568497e-03],\n",
       "       [9.99253333e-01],\n",
       "       [7.51564919e-04],\n",
       "       [9.99966562e-01],\n",
       "       [9.99951184e-01],\n",
       "       [3.92985094e-04],\n",
       "       [7.95614719e-01],\n",
       "       [7.86122400e-04],\n",
       "       [5.48191776e-04],\n",
       "       [2.94785559e-01],\n",
       "       [9.99922454e-01],\n",
       "       [4.51491401e-03],\n",
       "       [6.26520114e-03],\n",
       "       [8.31723737e-05],\n",
       "       [4.53184685e-03],\n",
       "       [2.65881099e-05],\n",
       "       [9.99942362e-01],\n",
       "       [9.99717772e-01],\n",
       "       [9.99945343e-01],\n",
       "       [9.99035120e-01],\n",
       "       [9.96954024e-01],\n",
       "       [3.42181930e-03],\n",
       "       [7.10239401e-05],\n",
       "       [9.92936194e-01],\n",
       "       [9.99803662e-01],\n",
       "       [9.99947488e-01],\n",
       "       [1.62986282e-04],\n",
       "       [1.14938105e-03],\n",
       "       [1.64146797e-04],\n",
       "       [9.99952078e-01],\n",
       "       [9.99943912e-01],\n",
       "       [2.50569923e-04],\n",
       "       [4.21726435e-01],\n",
       "       [9.99975920e-01],\n",
       "       [1.39266311e-03],\n",
       "       [9.73784737e-03],\n",
       "       [9.99419630e-01],\n",
       "       [2.45339994e-04],\n",
       "       [2.92176788e-04],\n",
       "       [9.99377549e-01],\n",
       "       [7.99662247e-02],\n",
       "       [1.71135873e-01],\n",
       "       [9.99982536e-01],\n",
       "       [1.57596834e-04],\n",
       "       [1.26082613e-03],\n",
       "       [4.23003476e-05],\n",
       "       [9.44767671e-05],\n",
       "       [2.79958622e-04],\n",
       "       [9.99939263e-01],\n",
       "       [2.23862662e-04],\n",
       "       [3.83033782e-01],\n",
       "       [9.98778105e-01],\n",
       "       [1.69319939e-02],\n",
       "       [2.69779866e-03],\n",
       "       [4.99747366e-05],\n",
       "       [2.21670707e-04],\n",
       "       [9.99966264e-01],\n",
       "       [9.99918520e-01],\n",
       "       [8.15578562e-04],\n",
       "       [1.75941255e-04],\n",
       "       [8.17156630e-04],\n",
       "       [1.34128364e-04],\n",
       "       [9.99892354e-01],\n",
       "       [2.34331470e-04],\n",
       "       [9.99873221e-01],\n",
       "       [9.99739528e-01],\n",
       "       [9.91418898e-01],\n",
       "       [9.98544753e-01],\n",
       "       [7.80856609e-03],\n",
       "       [8.74337852e-01],\n",
       "       [2.80998851e-04],\n",
       "       [4.95869555e-02],\n",
       "       [7.83463776e-01],\n",
       "       [5.49639225e-01],\n",
       "       [1.25516858e-02],\n",
       "       [4.24900616e-04],\n",
       "       [8.66388291e-05],\n",
       "       [2.04641707e-04],\n",
       "       [1.99992512e-03],\n",
       "       [3.52004319e-01],\n",
       "       [1.22354645e-02],\n",
       "       [9.99960363e-01],\n",
       "       [9.99905169e-01],\n",
       "       [7.34113693e-01],\n",
       "       [9.99926090e-01],\n",
       "       [4.64267470e-03],\n",
       "       [3.29597970e-04],\n",
       "       [9.99906123e-01],\n",
       "       [6.78627379e-03],\n",
       "       [5.64470538e-04],\n",
       "       [2.31873274e-01],\n",
       "       [9.99442160e-01],\n",
       "       [6.33453819e-05],\n",
       "       [9.37970400e-01],\n",
       "       [9.99535024e-01],\n",
       "       [9.99882460e-01],\n",
       "       [9.99908388e-01],\n",
       "       [4.60662559e-04],\n",
       "       [5.81141561e-03],\n",
       "       [9.98399317e-01],\n",
       "       [8.76858467e-05],\n",
       "       [1.54131697e-03],\n",
       "       [7.72053674e-02],\n",
       "       [9.99778926e-01],\n",
       "       [9.97306824e-01],\n",
       "       [7.64280732e-04],\n",
       "       [3.64548527e-04],\n",
       "       [9.99295771e-01],\n",
       "       [2.60214991e-04],\n",
       "       [4.97993256e-04],\n",
       "       [5.01522372e-05],\n",
       "       [3.41795325e-01],\n",
       "       [9.99959707e-01],\n",
       "       [9.99888659e-01],\n",
       "       [8.36519059e-04],\n",
       "       [9.99950588e-01],\n",
       "       [9.99946117e-01],\n",
       "       [1.02110680e-04],\n",
       "       [9.99933720e-01],\n",
       "       [1.92998908e-02],\n",
       "       [9.90075290e-01],\n",
       "       [1.06125576e-02],\n",
       "       [3.68232111e-04],\n",
       "       [1.40111282e-04],\n",
       "       [1.45701604e-04],\n",
       "       [6.49249123e-04],\n",
       "       [1.11099027e-04],\n",
       "       [3.81983548e-01],\n",
       "       [7.84983102e-04],\n",
       "       [9.99942660e-01],\n",
       "       [3.65873129e-04],\n",
       "       [9.99136806e-01],\n",
       "       [9.99143243e-01],\n",
       "       [4.68325656e-04],\n",
       "       [2.29228986e-04],\n",
       "       [9.99965549e-01],\n",
       "       [4.46592196e-04],\n",
       "       [9.99939561e-01],\n",
       "       [2.68574089e-01],\n",
       "       [2.52749259e-03],\n",
       "       [9.68455300e-02],\n",
       "       [2.54602404e-04],\n",
       "       [1.09299077e-02],\n",
       "       [9.99946117e-01],\n",
       "       [9.07497742e-05],\n",
       "       [2.89258751e-04],\n",
       "       [7.06930552e-03],\n",
       "       [9.99951601e-01],\n",
       "       [8.54667369e-03],\n",
       "       [2.02271156e-04],\n",
       "       [9.99818563e-01],\n",
       "       [8.72825767e-05],\n",
       "       [3.20401479e-04],\n",
       "       [1.70373009e-04],\n",
       "       [2.19905488e-02],\n",
       "       [5.44976327e-04],\n",
       "       [3.21410865e-01],\n",
       "       [4.09726426e-03],\n",
       "       [2.41711340e-03],\n",
       "       [1.01942765e-04],\n",
       "       [1.00241508e-03],\n",
       "       [4.98675254e-05],\n",
       "       [9.99921203e-01],\n",
       "       [9.99247730e-01],\n",
       "       [1.54300628e-03],\n",
       "       [1.10769615e-04],\n",
       "       [4.26740153e-03],\n",
       "       [9.99950767e-01],\n",
       "       [9.70844328e-01],\n",
       "       [9.99976575e-01],\n",
       "       [4.71731852e-04],\n",
       "       [9.99868512e-01],\n",
       "       [1.68070823e-04],\n",
       "       [9.99570310e-01],\n",
       "       [9.94081855e-01],\n",
       "       [8.99294027e-05],\n",
       "       [9.99958634e-01],\n",
       "       [1.11185829e-03],\n",
       "       [9.99557734e-01],\n",
       "       [9.99982297e-01],\n",
       "       [2.68637785e-03],\n",
       "       [2.54894840e-04],\n",
       "       [3.66933167e-01],\n",
       "       [1.42881298e-04],\n",
       "       [9.99112368e-01],\n",
       "       [9.99936759e-01],\n",
       "       [2.25373641e-01],\n",
       "       [9.99914706e-01],\n",
       "       [1.34951016e-02],\n",
       "       [9.99918103e-01],\n",
       "       [9.24518108e-01],\n",
       "       [4.85613151e-03],\n",
       "       [8.35159444e-05],\n",
       "       [9.99803305e-01],\n",
       "       [1.27474807e-04],\n",
       "       [1.09459557e-01],\n",
       "       [9.99962330e-01],\n",
       "       [9.99832571e-01],\n",
       "       [9.99960124e-01],\n",
       "       [9.99878705e-01],\n",
       "       [7.24090159e-01],\n",
       "       [9.48026776e-01],\n",
       "       [1.11784335e-04],\n",
       "       [9.05835211e-01],\n",
       "       [9.99839842e-01],\n",
       "       [9.99917686e-01],\n",
       "       [2.65196926e-04],\n",
       "       [9.99856651e-01],\n",
       "       [9.99933422e-01],\n",
       "       [1.33369444e-03],\n",
       "       [2.15313723e-03],\n",
       "       [1.25516858e-02],\n",
       "       [3.56883014e-04],\n",
       "       [9.73036945e-01],\n",
       "       [9.93010402e-01],\n",
       "       [9.99953330e-01],\n",
       "       [3.05929076e-04],\n",
       "       [1.03497820e-04],\n",
       "       [1.64811616e-04],\n",
       "       [8.61923024e-02],\n",
       "       [5.01862541e-03],\n",
       "       [5.39338263e-03],\n",
       "       [9.97273088e-01],\n",
       "       [8.76784834e-05],\n",
       "       [1.18617661e-01],\n",
       "       [1.27170920e-01],\n",
       "       [2.82112346e-03],\n",
       "       [9.99899924e-01],\n",
       "       [8.13919876e-04],\n",
       "       [9.99848545e-01],\n",
       "       [6.08975999e-04],\n",
       "       [4.95651366e-05],\n",
       "       [2.14752552e-04],\n",
       "       [9.99957800e-01],\n",
       "       [9.71531272e-01],\n",
       "       [1.67081424e-04],\n",
       "       [3.13487314e-02],\n",
       "       [8.02095234e-01],\n",
       "       [9.19675731e-05],\n",
       "       [9.99925256e-01],\n",
       "       [4.87677666e-04],\n",
       "       [9.27149415e-01],\n",
       "       [9.89009798e-01],\n",
       "       [2.91174569e-04],\n",
       "       [2.24719390e-01],\n",
       "       [3.99802402e-02],\n",
       "       [1.16296706e-03],\n",
       "       [9.99924481e-01],\n",
       "       [4.46399953e-03],\n",
       "       [8.46430659e-01],\n",
       "       [9.99971271e-01],\n",
       "       [4.87436265e-01],\n",
       "       [2.33952049e-02],\n",
       "       [6.33453819e-05],\n",
       "       [1.17036454e-01],\n",
       "       [9.99483883e-01],\n",
       "       [9.99962509e-01],\n",
       "       [8.81081462e-01],\n",
       "       [6.56550610e-03],\n",
       "       [9.99951005e-01],\n",
       "       [9.86845493e-02],\n",
       "       [9.99935567e-01],\n",
       "       [9.86845493e-02],\n",
       "       [9.99948084e-01],\n",
       "       [8.37451444e-05],\n",
       "       [6.28659781e-03],\n",
       "       [8.37455373e-05],\n",
       "       [9.99961793e-01],\n",
       "       [1.58920931e-03],\n",
       "       [2.98899628e-04],\n",
       "       [1.23048900e-04],\n",
       "       [4.92478488e-04],\n",
       "       [7.61716976e-04],\n",
       "       [3.69201734e-04],\n",
       "       [1.57157471e-03],\n",
       "       [5.55040897e-04],\n",
       "       [4.59405506e-04],\n",
       "       [9.99826133e-01],\n",
       "       [9.21143568e-04],\n",
       "       [2.24927557e-03],\n",
       "       [5.01550210e-04],\n",
       "       [9.70859692e-05],\n",
       "       [5.41174151e-02],\n",
       "       [9.99830723e-01],\n",
       "       [9.03825625e-04],\n",
       "       [2.10696071e-01],\n",
       "       [1.36683811e-03],\n",
       "       [9.99768734e-01],\n",
       "       [9.99926925e-01],\n",
       "       [1.06751169e-04],\n",
       "       [1.28134474e-04],\n",
       "       [1.39090029e-04],\n",
       "       [4.07450971e-05],\n",
       "       [9.99937654e-01],\n",
       "       [9.70859692e-05],\n",
       "       [9.56601929e-04],\n",
       "       [9.99892354e-01],\n",
       "       [9.99942780e-01],\n",
       "       [9.99948025e-01],\n",
       "       [9.99979615e-01],\n",
       "       [9.99993026e-01],\n",
       "       [2.49196193e-04],\n",
       "       [1.30063956e-04],\n",
       "       [1.82596054e-02],\n",
       "       [9.99625862e-01],\n",
       "       [9.99958754e-01],\n",
       "       [9.46979642e-01],\n",
       "       [4.49486601e-04],\n",
       "       [9.99805331e-01],\n",
       "       [8.13333094e-01],\n",
       "       [7.95407032e-05],\n",
       "       [1.08862972e-04],\n",
       "       [1.63722865e-03],\n",
       "       [7.21404189e-03],\n",
       "       [1.08604479e-04],\n",
       "       [8.43098969e-04],\n",
       "       [9.93555725e-01],\n",
       "       [9.99976158e-01],\n",
       "       [2.25244265e-04],\n",
       "       [9.99940932e-01],\n",
       "       [9.99763429e-01],\n",
       "       [1.46669615e-02],\n",
       "       [5.48000773e-03],\n",
       "       [9.04186666e-02],\n",
       "       [6.36593640e-01],\n",
       "       [2.08254857e-03],\n",
       "       [1.34483475e-04]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meking predictions\n",
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting model 3 pred probs to label format\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.50918635170603,\n",
       " 'precision': 0.766006698423534,\n",
       " 'recall': 0.7650918635170604,\n",
       " 'f1': 0.7632813381351602}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model 3 results\n",
    "model_3_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_3_preds\n",
    ")\n",
    "model_3_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_bidir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_bidir\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Checking model details\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_4.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_4_bidir/20230114-161903\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 11s 30ms/step - loss: 0.1062 - accuracy: 0.9698 - val_loss: 0.9913 - val_accuracy: 0.7612\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.0538 - accuracy: 0.9777 - val_loss: 1.1466 - val_accuracy: 0.7625\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 6s 26ms/step - loss: 0.0484 - accuracy: 0.9774 - val_loss: 1.3015 - val_accuracy: 0.7703\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 6s 28ms/step - loss: 0.0485 - accuracy: 0.9787 - val_loss: 1.1624 - val_accuracy: 0.7664\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 6s 28ms/step - loss: 0.0403 - accuracy: 0.9812 - val_loss: 1.2603 - val_accuracy: 0.7703\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "history_4 = model_4.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_4_bidir\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.2353349e-03],\n",
       "       [7.3685712e-01],\n",
       "       [9.9996543e-01],\n",
       "       [2.0454343e-01],\n",
       "       [5.8164722e-05],\n",
       "       [9.9957860e-01],\n",
       "       [7.8354985e-02],\n",
       "       [9.9998742e-01],\n",
       "       [9.9996710e-01],\n",
       "       [9.9798030e-01]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 0., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusting model's probs to the label format\n",
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.03412073490814,\n",
       " 'precision': 0.7713700595744248,\n",
       " 'recall': 0.7703412073490814,\n",
       " 'f1': 0.768571140634933}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking results of predictions\n",
    "model_4_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_4_preds\n",
    ")\n",
    "model_4_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 - Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing embedding layer\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding_test = embedding(text_vectorizer(\"This is a test sentence\"))\n",
    "# print(embedding_test.shape)\n",
    "embedding_test = tf.expand_dims(embedding_test, axis=0)\n",
    "# print(embedding_test.shape)\n",
    "conv_1d = layers.Conv1D(\n",
    "    filters=32,\n",
    "    kernel_size=5,\n",
    "    activation=\"relu\",\n",
    "    padding=\"valid\"\n",
    ")\n",
    "conv_1d_output = conv_1d(embedding_test)\n",
    "max_pool = layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output)\n",
    "\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 - 1D Convolutional Network layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"input_layer\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(\n",
    "    filters=64,\n",
    "    kernel_size=5,\n",
    "    strides=1,\n",
    "    activation=\"relu\",\n",
    "    padding=\"valid\"\n",
    ")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_conv1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_conv1d\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 11, 64)            41024     \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,321,089\n",
      "Trainable params: 1,321,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Checking model details\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_5.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_5_conv1d/20230114-235045\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0439 - accuracy: 0.9807 - val_loss: 1.2497 - val_accuracy: 0.7585\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0415 - accuracy: 0.9816 - val_loss: 1.2930 - val_accuracy: 0.7480\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0409 - accuracy: 0.9812 - val_loss: 1.3431 - val_accuracy: 0.7441\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0401 - accuracy: 0.9822 - val_loss: 1.3221 - val_accuracy: 0.7467\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0414 - accuracy: 0.9810 - val_loss: 1.2909 - val_accuracy: 0.7480\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "history_5 = model_5.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_5_conv1d\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8230a386d8e0083990873cddb8ebb5b6213275a10339230a8504f0ef8ce7f888"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
