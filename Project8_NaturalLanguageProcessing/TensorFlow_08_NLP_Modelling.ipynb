{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "# Importing TF and checking the version\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing helper functions\n",
    "from DanielBourke_HelperFunctions import create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"NLP_text/train.csv\")\n",
    "test_df = pd.read_csv(\"NLP_text/test.csv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffling training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking test dataframe\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking number of training records\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking total number of samples\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Officially skipping out on #FantasticFour/#Fant4stic/whatever the hashtag is. It's getting ANNIHILATED in reviews. Bummer.\n",
      "---\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "I liked a @YouTube video http://t.co/5fR41TPzte Thorin's Thoughts - Riot and Sandbox Mode (LoL)\n",
      "---\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Parents are taking their kids to Burning Man and one 11 year old thinks it's 'better than... http://t.co/wp6V1BHhoQ\n",
      "---\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Learn How I Gained Access To The Secrets Of The Top Earners &amp; Used Them To Explode My Home Business Here: http://t.co/8rABhQrTh5 Please #RT\n",
      "---\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "HereÛªs how media in Pakistan covered the capture of terrorist Mohammed Naved http://t.co/f7WqpCEkg2\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Visualising random samples\n",
    "import random\n",
    "\n",
    "random_index = random.randint(0, len(train_df) - 5)\n",
    "\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index : random_index + 5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    train_df_shuffled[\"text\"].to_numpy(),\n",
    "    train_df_shuffled[\"target\"].to_numpy(),\n",
    "    test_size=0.1, # Allocating 10% to validation data\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking dataset length\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the first 10 samples\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using text vectorisation\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=None,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    ngrams=None,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    "    pad_to_max_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the average number of tokens\n",
    "round(sum([len(i.split()) for i in train_sentences]) / len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up text vectorisation variables\n",
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting text vectorisation to the training dataset\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 74,   9,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample sentnence and tokenise it\n",
    "sample_sentence = \"There is a flood in my street\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/yNXnvVKCDA | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/weQPesENku,\n",
      "tokenised version:\n",
      "[[2582 2420 2428  966    1 2490 2133 2249 2138 1685 1307 2427    1    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenising random sentence from the training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(\n",
    "    f\"Original text:\\n{random_sentence},\\ntokenised version:\\n{text_vectorizer([random_sentence])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " ['', '[UNK]', 'the', 'a', 'in'],\n",
       " ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "\n",
    "len(words_in_vocab), top_5_words, bottom_5_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x19ca5941c40>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the layer\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length\n",
    ")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "@RosemaryTravale Do we all use the same weapon? 'cause we might be screwed XD,\n",
      "embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.02209502,  0.04833481,  0.02057297, ..., -0.0417004 ,\n",
       "          0.01724714,  0.00334507],\n",
       "        [ 0.0044282 ,  0.02959353, -0.02985519, ..., -0.00032227,\n",
       "          0.03369289, -0.0301164 ],\n",
       "        [ 0.02713615, -0.00702845, -0.00376241, ..., -0.01400626,\n",
       "         -0.0199459 , -0.00800296],\n",
       "        ...,\n",
       "        [-0.00922608, -0.04281855,  0.00149045, ..., -0.01199614,\n",
       "          0.04132703, -0.00422592],\n",
       "        [ 0.02289363, -0.01219679, -0.04508603, ...,  0.01462224,\n",
       "          0.0351765 , -0.02655279],\n",
       "        [ 0.04544933, -0.00372183,  0.03552462, ..., -0.03026444,\n",
       "         -0.0371375 ,  0.02856531]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(\n",
    "    f\"Original text:\\n{random_sentence},\\nembedded version:\"\n",
    ")\n",
    "\n",
    "# Embed random sentence\n",
    "embed_sentence = embedding(text_vectorizer([random_sentence]))\n",
    "embed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.02209502,  0.04833481,  0.02057297,  0.032789  ,  0.04318057,\n",
       "        -0.03431219,  0.01098975,  0.03514931,  0.03978893,  0.01736709,\n",
       "        -0.02824736, -0.02608452, -0.00248503, -0.02237294,  0.0012821 ,\n",
       "        -0.00635846, -0.01773519,  0.03112418,  0.01703538, -0.02766299,\n",
       "         0.03034944,  0.03847185, -0.00869383, -0.04557556, -0.04246776,\n",
       "        -0.02699394,  0.03543986,  0.01170706, -0.02990152, -0.04815678,\n",
       "         0.01021283,  0.01171887, -0.02144183,  0.04833951,  0.02251032,\n",
       "         0.00594647, -0.0025661 , -0.04159676, -0.04176854, -0.00442631,\n",
       "         0.02404336, -0.04970976, -0.00243689,  0.03470664, -0.02492545,\n",
       "         0.00792212,  0.03716299, -0.02514007,  0.04799393,  0.04344555,\n",
       "         0.0493938 , -0.04462188, -0.02766787,  0.04958842, -0.01133138,\n",
       "         0.01706865,  0.04976422,  0.02401492, -0.0471971 , -0.00576581,\n",
       "        -0.00843783, -0.0460158 , -0.0047438 , -0.02876973, -0.01069454,\n",
       "         0.01619888,  0.04037999,  0.01838851, -0.00422462, -0.04191711,\n",
       "        -0.00661899, -0.0099126 ,  0.00550188,  0.00955259, -0.04527506,\n",
       "        -0.03952737,  0.00068357,  0.01506237,  0.02933272,  0.03145499,\n",
       "         0.00036741, -0.02838404,  0.03360294, -0.00809816, -0.03861867,\n",
       "        -0.01535642, -0.01991108, -0.00284731, -0.00102241,  0.00881507,\n",
       "         0.04184842,  0.02010867, -0.03618548,  0.03027866,  0.03916322,\n",
       "         0.00805066, -0.03710666, -0.04139948,  0.00847573,  0.02389066,\n",
       "         0.01628243,  0.00759467,  0.00631318, -0.03897363,  0.00185246,\n",
       "        -0.02116649, -0.01038554,  0.01984959,  0.0469817 ,  0.02188386,\n",
       "         0.01445884, -0.03468984,  0.04734352, -0.04908769, -0.00561147,\n",
       "        -0.01057572,  0.01363671,  0.00763376, -0.01649905,  0.00951245,\n",
       "         0.04764811, -0.01827   , -0.00726034,  0.04098501,  0.04254693,\n",
       "        -0.0417004 ,  0.01724714,  0.00334507], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " \"@RosemaryTravale Do we all use the same weapon? 'cause we might be screwed XD\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking single token's embedding\n",
    "embed_sentence[0][0], embed_sentence[0][0].shape, random_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using SKLearn to build base model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenisation and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # Convert words to numbers\n",
    "    (\"clf\", MultinomialNB()) # Model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved accuracy: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating baseline model\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Achieved accuracy: {baseline_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "baseline_predictions = model_0.predict(val_sentences)\n",
    "baseline_predictions[:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing SKLearn functions\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Developing function to evaluate accuracy, precision, recall and F1 scor\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate binary classification model\n",
    "    \"\"\"\n",
    "    # Calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    # Calculate model precision, recall and F1 score using \"weighted\" average\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\n",
    "        \"accuracy\": model_accuracy,\n",
    "        \"precision\": model_precision,\n",
    "        \"recall\": model_recall,\n",
    "        \"f1\": model_f1\n",
    "    }\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting baseline results\n",
    "baseline_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=baseline_predictions\n",
    ")\n",
    "baseline_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: a simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensorboard callback\n",
    "from DanielBourke_HelperFunctions import create_tensorboard_callback\n",
    "\n",
    "# Creating log directory\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with Functional API\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype=tf.string)\n",
    "x = text_vectorizer(inputs) # Turn the input text into numbers\n",
    "x = embedding(x) # Create embedding of numberised inputs\n",
    "x = layers.GlobalAveragePooling1D()(x) # Condence the feature vector\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Getting model summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model\n",
    "model_1.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_1_dense/20230116-103108\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 15ms/step - loss: 0.6101 - accuracy: 0.6920 - val_loss: 0.5347 - val_accuracy: 0.7651\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.4408 - accuracy: 0.8173 - val_loss: 0.4692 - val_accuracy: 0.7835\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.3457 - accuracy: 0.8608 - val_loss: 0.4593 - val_accuracy: 0.7874\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.2831 - accuracy: 0.8930 - val_loss: 0.4635 - val_accuracy: 0.7874\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.2358 - accuracy: 0.9123 - val_loss: 0.4821 - val_accuracy: 0.7848\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "history_1 = model_1.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_1_dense\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4821 - accuracy: 0.7848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48205941915512085, 0.7847769260406494]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the results\n",
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(762, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3532249 ],\n",
       "       [0.75160396],\n",
       "       [0.99763644],\n",
       "       [0.09741288],\n",
       "       [0.09976958],\n",
       "       [0.93916947],\n",
       "       [0.91113394],\n",
       "       [0.9932434 ],\n",
       "       [0.9641323 ],\n",
       "       [0.26156282]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 predictions\n",
    "model_1_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting model predictions to label format\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.4776902887139,\n",
       " 'precision': 0.790955383689072,\n",
       " 'recall': 0.7847769028871391,\n",
       " 'f1': 0.7812916448740085}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating model 1 results\n",
    "model_1_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_1_preds\n",
    ")\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving baseline results\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing model results\n",
    "import numpy as np\n",
    "\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting vocabulary from the text vectorisation layer\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Checking model 1 details\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.03117545, -0.0198781 ,  0.04902324, ..., -0.04492381,\n",
       "          -0.05209893,  0.04254517],\n",
       "         [ 0.02417207, -0.01064174,  0.00071459, ..., -0.01533931,\n",
       "          -0.0388462 ,  0.04234987],\n",
       "         [-0.0505834 , -0.02030688,  0.00483891, ..., -0.03868733,\n",
       "          -0.0411087 ,  0.04079808],\n",
       "         ...,\n",
       "         [ 0.04937395, -0.0193275 ,  0.01589176, ...,  0.01257899,\n",
       "           0.04665932, -0.00394964],\n",
       "         [ 0.01028025, -0.03333158,  0.07804684, ..., -0.06535052,\n",
       "          -0.07028744,  0.03675681],\n",
       "         [-0.02590463, -0.08515803,  0.05049384, ..., -0.10260323,\n",
       "          -0.10310195,  0.11368717]], dtype=float32)],\n",
       " (10000, 128))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the weights\n",
    "embed_weights = model_1.get_layer(\"embedding\").get_weights()\n",
    "embed_weights, embed_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embedding files (sample from TF Word Embedding documentation)\n",
    "import io\n",
    "\n",
    "out_v = io.open(\"vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for index, word in enumerate(words_in_vocab):\n",
    "    if index == 0:\n",
    "        continue # Skip 0, as it's a padding\n",
    "    vec = embed_weights[0][index]\n",
    "    out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated vectors.tsv and metadata.tsv can be uploaded into Tensorflow Embedding Projector tool at https://projector.tensorflow.org/ to visualise weights in 3D space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - RNN (Recurrent Neural Network) with LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15, 128)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "# CReating an RNN model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "print(x.shape)\n",
    "# x = layers.LSTM(units=64, return_sequences=True)(x) # Sequences required when you stack LSTM layers\n",
    "# print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "print(x.shape)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Checking model's summary\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_2.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_2_LSTM/20230116-103129\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 18ms/step - loss: 0.2247 - accuracy: 0.9187 - val_loss: 0.5958 - val_accuracy: 0.7835\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1560 - accuracy: 0.9423 - val_loss: 0.6173 - val_accuracy: 0.7874\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.1303 - accuracy: 0.9511 - val_loss: 0.7270 - val_accuracy: 0.7795\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.1080 - accuracy: 0.9578 - val_loss: 0.7674 - val_accuracy: 0.7782\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0856 - accuracy: 0.9666 - val_loss: 1.0402 - val_accuracy: 0.7795\n"
     ]
    }
   ],
   "source": [
    "# Fittting the model\n",
    "history_2 = model_2.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_2_LSTM\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.08379915e-01],\n",
       "       [7.43641078e-01],\n",
       "       [9.99641776e-01],\n",
       "       [2.65806187e-02],\n",
       "       [2.02441894e-04],\n",
       "       [9.99169886e-01],\n",
       "       [9.18037057e-01],\n",
       "       [9.99792099e-01],\n",
       "       [9.99619246e-01],\n",
       "       [6.23820305e-01]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meking predictions\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting pred probs to label format\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.95275590551181,\n",
       " 'precision': 0.7799065773530309,\n",
       " 'recall': 0.7795275590551181,\n",
       " 'f1': 0.7783167829714759}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate model 2 results\n",
    "model_2_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_2_preds\n",
    ")\n",
    "model_2_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - RNN with GRU (gated recurrent unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "# x = layers.GRU(64, return_sequences=True)(x) # Sequences, if GRU are stuck on each other\n",
    "x = layers.GRU(64)(x)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x)\n",
    "# x = layers.GlobalAveragePooling1D()(x) # If we want to consolidate GRU layer with sequences\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,317,313\n",
      "Trainable params: 1,317,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Getting model summary\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_3.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_3_GRU/20230116-103150\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 19ms/step - loss: 0.1537 - accuracy: 0.9426 - val_loss: 0.6736 - val_accuracy: 0.7769\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.0831 - accuracy: 0.9686 - val_loss: 0.9990 - val_accuracy: 0.7782\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0716 - accuracy: 0.9726 - val_loss: 0.8443 - val_accuracy: 0.7690\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.0626 - accuracy: 0.9753 - val_loss: 0.9545 - val_accuracy: 0.7756\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0564 - accuracy: 0.9766 - val_loss: 0.9678 - val_accuracy: 0.7743\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "history_3 = model_3.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_3_GRU\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.65900637e-03],\n",
       "       [7.43340492e-01],\n",
       "       [9.99801517e-01],\n",
       "       [1.59227267e-01],\n",
       "       [2.54200248e-04],\n",
       "       [9.99493241e-01],\n",
       "       [6.09049141e-01],\n",
       "       [9.99903500e-01],\n",
       "       [9.99839962e-01],\n",
       "       [5.78665257e-01],\n",
       "       [8.11867532e-04],\n",
       "       [8.48473966e-01],\n",
       "       [6.93586306e-04],\n",
       "       [1.41044393e-01],\n",
       "       [2.36695734e-04],\n",
       "       [8.22120905e-03],\n",
       "       [2.15882156e-03],\n",
       "       [6.05901878e-04],\n",
       "       [5.87642156e-02],\n",
       "       [9.99629200e-01],\n",
       "       [9.98125613e-01],\n",
       "       [1.13471207e-04],\n",
       "       [9.99618411e-01],\n",
       "       [7.02270400e-03],\n",
       "       [9.99822915e-01],\n",
       "       [9.99863327e-01],\n",
       "       [5.19790500e-03],\n",
       "       [2.83036754e-03],\n",
       "       [1.12335628e-03],\n",
       "       [3.29441369e-01],\n",
       "       [9.54154134e-01],\n",
       "       [5.31938858e-03],\n",
       "       [6.24567866e-01],\n",
       "       [4.92135668e-03],\n",
       "       [5.11452019e-01],\n",
       "       [1.45058215e-01],\n",
       "       [9.99590993e-01],\n",
       "       [3.01710695e-01],\n",
       "       [4.24492545e-02],\n",
       "       [9.99869883e-01],\n",
       "       [3.62982452e-01],\n",
       "       [1.12436137e-04],\n",
       "       [4.21873182e-02],\n",
       "       [5.58498548e-04],\n",
       "       [9.66013372e-01],\n",
       "       [9.99627709e-01],\n",
       "       [9.91083086e-01],\n",
       "       [9.53381538e-01],\n",
       "       [1.63908135e-02],\n",
       "       [1.38315156e-01],\n",
       "       [2.12455858e-02],\n",
       "       [5.53830564e-01],\n",
       "       [3.78064513e-02],\n",
       "       [2.05123406e-02],\n",
       "       [7.34864652e-01],\n",
       "       [2.74413936e-02],\n",
       "       [3.38559435e-03],\n",
       "       [9.99860585e-01],\n",
       "       [7.50579638e-04],\n",
       "       [1.58785318e-03],\n",
       "       [7.76392743e-02],\n",
       "       [9.99718189e-01],\n",
       "       [9.98519182e-01],\n",
       "       [3.21078338e-02],\n",
       "       [9.99894559e-01],\n",
       "       [9.99926925e-01],\n",
       "       [9.84398007e-01],\n",
       "       [4.84434515e-03],\n",
       "       [9.66421068e-01],\n",
       "       [6.74985349e-02],\n",
       "       [2.74847448e-02],\n",
       "       [6.13181107e-02],\n",
       "       [9.99893486e-01],\n",
       "       [2.85687689e-02],\n",
       "       [9.79159057e-01],\n",
       "       [9.93601799e-01],\n",
       "       [8.24452117e-02],\n",
       "       [9.99156713e-01],\n",
       "       [2.74658531e-01],\n",
       "       [3.13432008e-01],\n",
       "       [3.31751467e-03],\n",
       "       [1.26925424e-01],\n",
       "       [9.99890566e-01],\n",
       "       [3.78762488e-03],\n",
       "       [6.18112460e-03],\n",
       "       [9.26626194e-03],\n",
       "       [1.86900736e-03],\n",
       "       [1.26119249e-03],\n",
       "       [8.48055910e-03],\n",
       "       [9.99647975e-01],\n",
       "       [9.99662220e-01],\n",
       "       [2.25073600e-04],\n",
       "       [9.92328823e-01],\n",
       "       [3.76242620e-04],\n",
       "       [9.99877810e-01],\n",
       "       [7.68147826e-01],\n",
       "       [8.33592474e-01],\n",
       "       [9.99915242e-01],\n",
       "       [9.98961568e-01],\n",
       "       [9.98940587e-01],\n",
       "       [9.99968052e-01],\n",
       "       [4.43128236e-02],\n",
       "       [2.65183276e-04],\n",
       "       [9.76342201e-01],\n",
       "       [9.99182343e-01],\n",
       "       [9.19529945e-02],\n",
       "       [9.90171909e-01],\n",
       "       [9.86572444e-01],\n",
       "       [3.65298986e-03],\n",
       "       [9.99602318e-01],\n",
       "       [9.95956361e-01],\n",
       "       [4.38304356e-04],\n",
       "       [5.86814225e-01],\n",
       "       [4.38615168e-03],\n",
       "       [1.53615139e-03],\n",
       "       [1.47875190e-01],\n",
       "       [8.17322850e-01],\n",
       "       [9.99644160e-01],\n",
       "       [6.35403618e-02],\n",
       "       [1.17174489e-03],\n",
       "       [9.99898911e-01],\n",
       "       [8.16005690e-04],\n",
       "       [4.68307815e-04],\n",
       "       [8.66317034e-01],\n",
       "       [2.03497201e-01],\n",
       "       [1.26205878e-02],\n",
       "       [9.99659240e-01],\n",
       "       [1.90226972e-04],\n",
       "       [7.46094063e-03],\n",
       "       [9.88375545e-01],\n",
       "       [1.50220124e-02],\n",
       "       [9.99898911e-01],\n",
       "       [9.99935925e-01],\n",
       "       [9.99863327e-01],\n",
       "       [9.99618530e-01],\n",
       "       [7.90537242e-03],\n",
       "       [9.99752462e-01],\n",
       "       [1.48192659e-01],\n",
       "       [1.35735087e-02],\n",
       "       [4.24454827e-03],\n",
       "       [9.99929249e-01],\n",
       "       [8.01396847e-01],\n",
       "       [1.41044393e-01],\n",
       "       [9.99393284e-01],\n",
       "       [1.47373319e-01],\n",
       "       [2.48078741e-02],\n",
       "       [4.32704110e-04],\n",
       "       [1.46117323e-04],\n",
       "       [1.15939058e-01],\n",
       "       [9.99707580e-01],\n",
       "       [5.70361083e-03],\n",
       "       [4.13177945e-02],\n",
       "       [3.60489972e-02],\n",
       "       [7.16456678e-04],\n",
       "       [1.48564100e-03],\n",
       "       [9.99544501e-01],\n",
       "       [8.78073871e-01],\n",
       "       [2.97676120e-02],\n",
       "       [9.96147096e-01],\n",
       "       [7.74014217e-04],\n",
       "       [9.99387920e-01],\n",
       "       [6.70554629e-03],\n",
       "       [1.26810342e-01],\n",
       "       [9.88133073e-01],\n",
       "       [1.65951908e-01],\n",
       "       [2.54254352e-04],\n",
       "       [9.99863148e-01],\n",
       "       [1.00414827e-01],\n",
       "       [9.99732196e-01],\n",
       "       [8.62737417e-01],\n",
       "       [9.99837279e-01],\n",
       "       [9.96129870e-01],\n",
       "       [9.99133050e-01],\n",
       "       [5.44146053e-04],\n",
       "       [9.99965727e-01],\n",
       "       [2.51766481e-03],\n",
       "       [2.88951360e-02],\n",
       "       [7.29802102e-02],\n",
       "       [9.41987514e-01],\n",
       "       [9.99847472e-01],\n",
       "       [1.84238749e-03],\n",
       "       [9.97928381e-01],\n",
       "       [9.99794543e-01],\n",
       "       [9.99823987e-01],\n",
       "       [9.99818802e-01],\n",
       "       [1.81577099e-03],\n",
       "       [2.38447310e-03],\n",
       "       [9.99967456e-01],\n",
       "       [2.79632921e-04],\n",
       "       [2.19428257e-04],\n",
       "       [5.93037484e-03],\n",
       "       [9.98785555e-01],\n",
       "       [1.93798647e-03],\n",
       "       [1.37037702e-03],\n",
       "       [1.08320126e-03],\n",
       "       [5.85473841e-03],\n",
       "       [9.55670490e-04],\n",
       "       [5.55254659e-03],\n",
       "       [9.99507666e-01],\n",
       "       [1.00728357e-02],\n",
       "       [7.26855025e-02],\n",
       "       [9.99800563e-01],\n",
       "       [9.99758303e-01],\n",
       "       [7.60832429e-03],\n",
       "       [9.53910232e-04],\n",
       "       [9.99782503e-01],\n",
       "       [9.99619246e-01],\n",
       "       [9.99406934e-01],\n",
       "       [9.96853352e-01],\n",
       "       [9.87792313e-01],\n",
       "       [4.19320583e-01],\n",
       "       [9.99754071e-01],\n",
       "       [9.39030375e-04],\n",
       "       [8.93588457e-03],\n",
       "       [3.91916983e-04],\n",
       "       [3.51985625e-04],\n",
       "       [9.99873579e-01],\n",
       "       [9.75956440e-01],\n",
       "       [9.96178627e-01],\n",
       "       [3.06425959e-01],\n",
       "       [3.70756239e-02],\n",
       "       [3.69591918e-03],\n",
       "       [4.35136110e-02],\n",
       "       [1.04754651e-02],\n",
       "       [9.99697626e-01],\n",
       "       [2.41730615e-01],\n",
       "       [6.77269325e-02],\n",
       "       [9.99948263e-01],\n",
       "       [9.95463014e-01],\n",
       "       [8.77227366e-01],\n",
       "       [8.71543016e-04],\n",
       "       [1.73423402e-02],\n",
       "       [9.98104870e-01],\n",
       "       [2.89785564e-01],\n",
       "       [3.30343574e-01],\n",
       "       [4.14431058e-02],\n",
       "       [3.06457132e-01],\n",
       "       [5.90655580e-02],\n",
       "       [2.93271313e-03],\n",
       "       [1.60801469e-03],\n",
       "       [9.98492539e-01],\n",
       "       [3.84124070e-02],\n",
       "       [9.99790132e-01],\n",
       "       [9.99708354e-01],\n",
       "       [2.22107675e-03],\n",
       "       [3.24023189e-04],\n",
       "       [9.99535143e-01],\n",
       "       [8.43256363e-04],\n",
       "       [2.95439381e-02],\n",
       "       [3.22821051e-01],\n",
       "       [2.75944709e-03],\n",
       "       [9.98657405e-01],\n",
       "       [1.86977137e-04],\n",
       "       [7.16161216e-03],\n",
       "       [9.99559045e-01],\n",
       "       [1.36679277e-01],\n",
       "       [9.99655128e-01],\n",
       "       [9.99940574e-01],\n",
       "       [1.50017917e-01],\n",
       "       [5.71608427e-04],\n",
       "       [5.64941438e-03],\n",
       "       [8.20476154e-04],\n",
       "       [9.49527384e-05],\n",
       "       [9.99388576e-01],\n",
       "       [9.99806404e-01],\n",
       "       [4.64439332e-01],\n",
       "       [9.99415338e-01],\n",
       "       [5.25834272e-03],\n",
       "       [5.21454029e-02],\n",
       "       [6.59475685e-04],\n",
       "       [1.55477563e-03],\n",
       "       [7.78373040e-04],\n",
       "       [9.99642968e-01],\n",
       "       [1.36361243e-02],\n",
       "       [3.22929613e-04],\n",
       "       [9.99451339e-01],\n",
       "       [6.95995754e-04],\n",
       "       [2.00866302e-03],\n",
       "       [9.99524355e-01],\n",
       "       [4.10777284e-04],\n",
       "       [5.36585972e-02],\n",
       "       [5.97998151e-04],\n",
       "       [9.99623358e-01],\n",
       "       [7.66598523e-01],\n",
       "       [4.83058542e-01],\n",
       "       [5.33826232e-01],\n",
       "       [9.98607457e-01],\n",
       "       [9.09330975e-03],\n",
       "       [9.99624193e-01],\n",
       "       [2.96902042e-02],\n",
       "       [9.10151720e-01],\n",
       "       [9.59180057e-01],\n",
       "       [9.63807642e-01],\n",
       "       [3.17042209e-02],\n",
       "       [2.37503834e-03],\n",
       "       [5.72427273e-01],\n",
       "       [1.01549700e-01],\n",
       "       [3.03682964e-02],\n",
       "       [6.16587652e-03],\n",
       "       [4.61929329e-02],\n",
       "       [1.01430880e-04],\n",
       "       [4.59301565e-03],\n",
       "       [3.85578983e-02],\n",
       "       [9.99870956e-01],\n",
       "       [2.81600910e-03],\n",
       "       [3.95964049e-02],\n",
       "       [1.05932772e-01],\n",
       "       [2.31286481e-01],\n",
       "       [1.33239850e-01],\n",
       "       [2.77442136e-03],\n",
       "       [6.24215871e-04],\n",
       "       [9.99377906e-01],\n",
       "       [2.96385705e-01],\n",
       "       [3.14046979e-01],\n",
       "       [9.99956787e-01],\n",
       "       [8.85538640e-04],\n",
       "       [9.27636921e-01],\n",
       "       [1.27177954e-01],\n",
       "       [5.26730949e-03],\n",
       "       [1.27313167e-01],\n",
       "       [8.61840846e-04],\n",
       "       [4.69797440e-02],\n",
       "       [9.98816609e-01],\n",
       "       [6.98679090e-01],\n",
       "       [9.99706149e-01],\n",
       "       [2.29566574e-01],\n",
       "       [7.80967879e-04],\n",
       "       [9.99815464e-01],\n",
       "       [9.71166592e-04],\n",
       "       [9.99862134e-01],\n",
       "       [1.17751965e-02],\n",
       "       [9.14294831e-03],\n",
       "       [9.99813676e-01],\n",
       "       [8.89203104e-04],\n",
       "       [9.44555213e-04],\n",
       "       [9.99767542e-01],\n",
       "       [1.20689604e-03],\n",
       "       [6.19350653e-03],\n",
       "       [9.56186414e-01],\n",
       "       [9.98341680e-01],\n",
       "       [4.89400467e-04],\n",
       "       [2.58336663e-02],\n",
       "       [9.99631822e-01],\n",
       "       [9.99396086e-01],\n",
       "       [9.99205232e-01],\n",
       "       [1.99728787e-01],\n",
       "       [9.58331704e-01],\n",
       "       [9.96988714e-01],\n",
       "       [1.94841344e-02],\n",
       "       [1.60618697e-03],\n",
       "       [7.13235363e-02],\n",
       "       [9.93529186e-02],\n",
       "       [4.54308785e-04],\n",
       "       [6.75724804e-01],\n",
       "       [2.21989721e-01],\n",
       "       [4.86503937e-04],\n",
       "       [9.99459684e-01],\n",
       "       [9.99863327e-01],\n",
       "       [9.99866903e-01],\n",
       "       [3.29013844e-03],\n",
       "       [2.46413440e-01],\n",
       "       [1.59342527e-01],\n",
       "       [1.73317343e-01],\n",
       "       [9.85234439e-01],\n",
       "       [3.45763385e-01],\n",
       "       [3.51317198e-04],\n",
       "       [5.00788400e-03],\n",
       "       [2.36349064e-03],\n",
       "       [9.98872936e-01],\n",
       "       [1.95825147e-03],\n",
       "       [5.50303888e-03],\n",
       "       [1.35608471e-03],\n",
       "       [2.33976659e-03],\n",
       "       [2.74840504e-01],\n",
       "       [9.78438079e-01],\n",
       "       [9.59551483e-02],\n",
       "       [1.31039228e-03],\n",
       "       [5.07808402e-02],\n",
       "       [4.19107685e-03],\n",
       "       [9.99884546e-01],\n",
       "       [9.98874605e-01],\n",
       "       [5.80780327e-01],\n",
       "       [9.49313700e-01],\n",
       "       [3.78384080e-04],\n",
       "       [9.55250382e-01],\n",
       "       [9.99843478e-01],\n",
       "       [9.68462229e-01],\n",
       "       [1.49921790e-01],\n",
       "       [9.99113619e-01],\n",
       "       [3.30367535e-02],\n",
       "       [9.99897718e-01],\n",
       "       [9.66433715e-03],\n",
       "       [7.19077513e-03],\n",
       "       [2.93970734e-01],\n",
       "       [4.77445237e-02],\n",
       "       [9.99692738e-01],\n",
       "       [1.22796588e-01],\n",
       "       [1.86309946e-04],\n",
       "       [1.51427952e-03],\n",
       "       [3.45763385e-01],\n",
       "       [9.99973297e-01],\n",
       "       [1.64336452e-04],\n",
       "       [2.35858575e-01],\n",
       "       [9.99944329e-01],\n",
       "       [9.03974651e-05],\n",
       "       [9.99898911e-01],\n",
       "       [3.15780751e-03],\n",
       "       [6.08992949e-03],\n",
       "       [8.91590025e-04],\n",
       "       [9.99611199e-01],\n",
       "       [9.99283731e-01],\n",
       "       [4.91907634e-03],\n",
       "       [9.26922483e-04],\n",
       "       [9.98989761e-01],\n",
       "       [9.99816775e-01],\n",
       "       [6.23096883e-01],\n",
       "       [1.30103179e-03],\n",
       "       [7.39298901e-03],\n",
       "       [2.66606748e-01],\n",
       "       [2.67962809e-03],\n",
       "       [9.99904990e-01],\n",
       "       [4.45858926e-01],\n",
       "       [9.99905229e-01],\n",
       "       [9.99736309e-01],\n",
       "       [5.02990782e-02],\n",
       "       [2.44185135e-01],\n",
       "       [1.34038040e-03],\n",
       "       [9.99379873e-01],\n",
       "       [9.94476974e-01],\n",
       "       [9.80290294e-01],\n",
       "       [1.14229051e-02],\n",
       "       [4.30737138e-02],\n",
       "       [5.09318197e-03],\n",
       "       [3.80939688e-04],\n",
       "       [6.68974668e-02],\n",
       "       [4.33977284e-02],\n",
       "       [9.20728564e-01],\n",
       "       [1.30320648e-02],\n",
       "       [9.99930143e-01],\n",
       "       [9.99896884e-01],\n",
       "       [9.33733396e-03],\n",
       "       [7.43340492e-01],\n",
       "       [3.37377675e-02],\n",
       "       [1.32522627e-03],\n",
       "       [3.82794529e-01],\n",
       "       [9.90820944e-01],\n",
       "       [1.24444319e-02],\n",
       "       [1.23680957e-01],\n",
       "       [3.08378047e-04],\n",
       "       [2.25007106e-02],\n",
       "       [6.58908393e-05],\n",
       "       [9.99883533e-01],\n",
       "       [9.97614145e-01],\n",
       "       [9.99830127e-01],\n",
       "       [9.64456439e-01],\n",
       "       [9.38266218e-01],\n",
       "       [3.27999480e-02],\n",
       "       [3.04972171e-04],\n",
       "       [5.90364695e-01],\n",
       "       [9.99122381e-01],\n",
       "       [9.99812722e-01],\n",
       "       [7.32124085e-04],\n",
       "       [1.58070587e-02],\n",
       "       [3.82712972e-03],\n",
       "       [9.99841034e-01],\n",
       "       [9.99805748e-01],\n",
       "       [1.19087438e-03],\n",
       "       [1.80123728e-02],\n",
       "       [9.99922574e-01],\n",
       "       [1.90599151e-02],\n",
       "       [1.64223164e-02],\n",
       "       [9.89281237e-01],\n",
       "       [3.36586381e-03],\n",
       "       [4.15596785e-03],\n",
       "       [9.93288457e-01],\n",
       "       [2.33424440e-01],\n",
       "       [5.62482893e-01],\n",
       "       [9.99952734e-01],\n",
       "       [7.80271599e-04],\n",
       "       [6.39227219e-03],\n",
       "       [1.97343776e-04],\n",
       "       [5.04213618e-04],\n",
       "       [2.43479689e-03],\n",
       "       [9.99825478e-01],\n",
       "       [1.17639673e-03],\n",
       "       [8.27426791e-01],\n",
       "       [7.32494414e-01],\n",
       "       [8.97518545e-02],\n",
       "       [1.85413361e-01],\n",
       "       [1.34963033e-04],\n",
       "       [7.65016582e-03],\n",
       "       [9.99896765e-01],\n",
       "       [9.99434114e-01],\n",
       "       [1.52337784e-02],\n",
       "       [1.49220496e-03],\n",
       "       [2.64542061e-03],\n",
       "       [1.93581660e-03],\n",
       "       [9.97307777e-01],\n",
       "       [2.08392646e-03],\n",
       "       [9.96826530e-01],\n",
       "       [9.98512268e-01],\n",
       "       [8.29594433e-01],\n",
       "       [9.79359508e-01],\n",
       "       [1.84100673e-01],\n",
       "       [4.46142405e-02],\n",
       "       [1.25633394e-02],\n",
       "       [1.07047260e-01],\n",
       "       [8.10388327e-01],\n",
       "       [3.30137640e-01],\n",
       "       [1.69084236e-01],\n",
       "       [1.08118937e-03],\n",
       "       [1.73085558e-04],\n",
       "       [2.57646549e-03],\n",
       "       [1.35090709e-01],\n",
       "       [1.02907248e-01],\n",
       "       [1.34053439e-01],\n",
       "       [9.99906719e-01],\n",
       "       [9.99128938e-01],\n",
       "       [7.68147826e-01],\n",
       "       [9.99721825e-01],\n",
       "       [5.38416319e-02],\n",
       "       [8.53916164e-03],\n",
       "       [9.99440730e-01],\n",
       "       [7.01006595e-03],\n",
       "       [2.26381491e-03],\n",
       "       [1.75659150e-01],\n",
       "       [9.31960404e-01],\n",
       "       [1.74454486e-04],\n",
       "       [9.43671465e-01],\n",
       "       [9.86037791e-01],\n",
       "       [9.98857200e-01],\n",
       "       [9.99724805e-01],\n",
       "       [2.95584872e-02],\n",
       "       [8.62695277e-02],\n",
       "       [9.81820226e-01],\n",
       "       [7.24967394e-04],\n",
       "       [8.16423669e-02],\n",
       "       [2.95469105e-01],\n",
       "       [9.51170802e-01],\n",
       "       [9.37964141e-01],\n",
       "       [6.65362226e-03],\n",
       "       [1.09335836e-02],\n",
       "       [7.37303972e-01],\n",
       "       [3.54187912e-03],\n",
       "       [1.33661786e-02],\n",
       "       [2.42533046e-04],\n",
       "       [3.03478777e-01],\n",
       "       [9.99896586e-01],\n",
       "       [9.97695088e-01],\n",
       "       [1.22362031e-02],\n",
       "       [9.99881923e-01],\n",
       "       [9.99851465e-01],\n",
       "       [2.44691735e-04],\n",
       "       [9.99730408e-01],\n",
       "       [7.04803318e-02],\n",
       "       [9.92509484e-01],\n",
       "       [2.42324844e-01],\n",
       "       [6.71779132e-03],\n",
       "       [1.99039909e-03],\n",
       "       [7.79389520e-04],\n",
       "       [7.81053258e-03],\n",
       "       [5.40390552e-04],\n",
       "       [9.50911343e-01],\n",
       "       [1.50192366e-03],\n",
       "       [9.99795556e-01],\n",
       "       [4.87046037e-03],\n",
       "       [9.85589862e-01],\n",
       "       [8.97103250e-01],\n",
       "       [2.54717446e-03],\n",
       "       [1.24281330e-03],\n",
       "       [9.99945998e-01],\n",
       "       [1.43675804e-02],\n",
       "       [9.99791205e-01],\n",
       "       [2.57191628e-01],\n",
       "       [5.51332273e-02],\n",
       "       [3.45249981e-01],\n",
       "       [4.21997998e-03],\n",
       "       [1.34887099e-01],\n",
       "       [9.99851465e-01],\n",
       "       [3.68474401e-04],\n",
       "       [2.55236751e-03],\n",
       "       [6.45922050e-02],\n",
       "       [9.99848068e-01],\n",
       "       [4.53443006e-02],\n",
       "       [1.91400154e-03],\n",
       "       [9.95189250e-01],\n",
       "       [2.35760366e-04],\n",
       "       [1.09115848e-03],\n",
       "       [3.47829843e-03],\n",
       "       [2.77887046e-01],\n",
       "       [2.01323256e-02],\n",
       "       [2.48164013e-01],\n",
       "       [4.44542527e-01],\n",
       "       [5.16131967e-02],\n",
       "       [5.89542498e-04],\n",
       "       [5.46580590e-02],\n",
       "       [5.83465619e-04],\n",
       "       [9.99202013e-01],\n",
       "       [9.87935722e-01],\n",
       "       [2.11839408e-01],\n",
       "       [6.70409587e-04],\n",
       "       [9.67433210e-03],\n",
       "       [9.99901772e-01],\n",
       "       [9.63554025e-01],\n",
       "       [9.99948382e-01],\n",
       "       [3.20194778e-03],\n",
       "       [9.99338925e-01],\n",
       "       [4.95183514e-03],\n",
       "       [9.77794468e-01],\n",
       "       [9.52109993e-01],\n",
       "       [5.71991026e-04],\n",
       "       [9.99898016e-01],\n",
       "       [3.64447162e-02],\n",
       "       [9.86559391e-01],\n",
       "       [9.99953389e-01],\n",
       "       [4.00377139e-02],\n",
       "       [7.97226268e-04],\n",
       "       [9.83894825e-01],\n",
       "       [9.51161201e-04],\n",
       "       [3.45867090e-02],\n",
       "       [9.99782503e-01],\n",
       "       [3.55331957e-01],\n",
       "       [9.99402225e-01],\n",
       "       [1.22727863e-01],\n",
       "       [9.99609530e-01],\n",
       "       [6.11567676e-01],\n",
       "       [7.47267082e-02],\n",
       "       [5.94431767e-04],\n",
       "       [9.91865039e-01],\n",
       "       [9.97029012e-04],\n",
       "       [1.87165618e-01],\n",
       "       [9.99870420e-01],\n",
       "       [9.95029986e-01],\n",
       "       [9.99889433e-01],\n",
       "       [9.99543250e-01],\n",
       "       [1.79627553e-01],\n",
       "       [9.92861688e-01],\n",
       "       [7.41518626e-04],\n",
       "       [7.86485076e-01],\n",
       "       [9.80922282e-01],\n",
       "       [9.99246716e-01],\n",
       "       [1.82969868e-03],\n",
       "       [9.94481504e-01],\n",
       "       [9.99828577e-01],\n",
       "       [3.02427392e-02],\n",
       "       [1.94387846e-02],\n",
       "       [1.69084236e-01],\n",
       "       [6.62129559e-03],\n",
       "       [6.14132226e-01],\n",
       "       [9.85855401e-01],\n",
       "       [9.99838531e-01],\n",
       "       [9.16386209e-03],\n",
       "       [5.68493211e-04],\n",
       "       [1.08712597e-03],\n",
       "       [1.42874062e-01],\n",
       "       [3.15998052e-03],\n",
       "       [5.51740974e-02],\n",
       "       [9.43416953e-01],\n",
       "       [4.43719618e-04],\n",
       "       [2.48740241e-01],\n",
       "       [3.21703367e-02],\n",
       "       [1.59569904e-01],\n",
       "       [9.99624312e-01],\n",
       "       [2.30688825e-02],\n",
       "       [9.98657703e-01],\n",
       "       [2.45745275e-02],\n",
       "       [1.45738843e-04],\n",
       "       [1.53805048e-03],\n",
       "       [9.99885082e-01],\n",
       "       [7.41263330e-01],\n",
       "       [6.21493557e-04],\n",
       "       [3.08238477e-01],\n",
       "       [2.51093239e-01],\n",
       "       [4.54025256e-04],\n",
       "       [9.99623001e-01],\n",
       "       [9.51355416e-03],\n",
       "       [9.83450890e-01],\n",
       "       [2.97832966e-01],\n",
       "       [2.23199860e-03],\n",
       "       [4.21127439e-01],\n",
       "       [2.85781711e-01],\n",
       "       [4.48736250e-02],\n",
       "       [9.99832273e-01],\n",
       "       [9.63164791e-02],\n",
       "       [2.71294385e-01],\n",
       "       [9.99543369e-01],\n",
       "       [5.01923203e-01],\n",
       "       [3.11117411e-01],\n",
       "       [1.74454486e-04],\n",
       "       [2.40271971e-01],\n",
       "       [7.33436525e-01],\n",
       "       [9.99898911e-01],\n",
       "       [9.92158055e-01],\n",
       "       [2.26505399e-02],\n",
       "       [9.99864876e-01],\n",
       "       [1.68725431e-01],\n",
       "       [9.99649942e-01],\n",
       "       [1.68725431e-01],\n",
       "       [9.99886453e-01],\n",
       "       [2.57257459e-04],\n",
       "       [3.06683220e-02],\n",
       "       [2.79764412e-04],\n",
       "       [9.99915898e-01],\n",
       "       [6.16974663e-03],\n",
       "       [1.68089662e-02],\n",
       "       [7.93870597e-04],\n",
       "       [1.51145570e-02],\n",
       "       [1.71761028e-03],\n",
       "       [3.87354335e-03],\n",
       "       [5.20607643e-02],\n",
       "       [7.80313089e-03],\n",
       "       [1.36244949e-02],\n",
       "       [9.99267876e-01],\n",
       "       [3.85147450e-03],\n",
       "       [4.76810001e-02],\n",
       "       [3.75918532e-03],\n",
       "       [4.13684640e-04],\n",
       "       [1.09907292e-01],\n",
       "       [9.96552467e-01],\n",
       "       [7.78193912e-03],\n",
       "       [7.81369284e-02],\n",
       "       [1.83398984e-02],\n",
       "       [9.99053001e-01],\n",
       "       [9.99675333e-01],\n",
       "       [1.74666126e-03],\n",
       "       [7.13473011e-04],\n",
       "       [9.96360439e-04],\n",
       "       [9.76892261e-05],\n",
       "       [9.99843955e-01],\n",
       "       [4.13684640e-04],\n",
       "       [1.16893537e-01],\n",
       "       [9.97960269e-01],\n",
       "       [9.99821246e-01],\n",
       "       [9.99819398e-01],\n",
       "       [9.99949753e-01],\n",
       "       [9.99988139e-01],\n",
       "       [2.05664337e-03],\n",
       "       [2.88540992e-04],\n",
       "       [7.18801990e-02],\n",
       "       [9.93908107e-01],\n",
       "       [9.99901772e-01],\n",
       "       [9.43233907e-01],\n",
       "       [2.26333775e-02],\n",
       "       [9.98766422e-01],\n",
       "       [6.57855272e-01],\n",
       "       [2.76312727e-04],\n",
       "       [6.99189608e-04],\n",
       "       [2.16140728e-02],\n",
       "       [5.03454059e-02],\n",
       "       [3.06498667e-04],\n",
       "       [1.38695491e-02],\n",
       "       [9.95391488e-01],\n",
       "       [9.99929070e-01],\n",
       "       [1.63439254e-03],\n",
       "       [9.99753892e-01],\n",
       "       [6.94158852e-01],\n",
       "       [6.79889321e-02],\n",
       "       [4.06674594e-02],\n",
       "       [8.87250528e-02],\n",
       "       [7.18318224e-01],\n",
       "       [7.83237070e-02],\n",
       "       [4.06696199e-04]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meking predictions\n",
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting model 3 pred probs to label format\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.42782152230971,\n",
       " 'precision': 0.7757380419380466,\n",
       " 'recall': 0.7742782152230971,\n",
       " 'f1': 0.7723566516531356}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model 3 results\n",
    "model_3_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_3_preds\n",
    ")\n",
    "model_3_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_bidir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_bidir\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Checking model details\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_4.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_4_bidir/20230116-103211\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 9s 22ms/step - loss: 0.1056 - accuracy: 0.9682 - val_loss: 0.9006 - val_accuracy: 0.7730\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0525 - accuracy: 0.9781 - val_loss: 1.1231 - val_accuracy: 0.7717\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0440 - accuracy: 0.9787 - val_loss: 1.4404 - val_accuracy: 0.7756\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0430 - accuracy: 0.9801 - val_loss: 1.5227 - val_accuracy: 0.7756\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0426 - accuracy: 0.9812 - val_loss: 1.2675 - val_accuracy: 0.7743\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "history_4 = model_4.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_4_bidir\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.2413623e-01],\n",
       "       [6.3701016e-01],\n",
       "       [9.9989742e-01],\n",
       "       [1.3428456e-01],\n",
       "       [1.6744674e-05],\n",
       "       [9.9727756e-01],\n",
       "       [8.9310348e-01],\n",
       "       [9.9995303e-01],\n",
       "       [9.9972731e-01],\n",
       "       [9.6627390e-01]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusting model's probs to the label format\n",
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.42782152230971,\n",
       " 'precision': 0.7745797612274115,\n",
       " 'recall': 0.7742782152230971,\n",
       " 'f1': 0.7730386111374632}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking results of predictions\n",
    "model_4_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_4_preds\n",
    ")\n",
    "model_4_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 - Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing embedding layer\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding_test = embedding(text_vectorizer(\"This is a test sentence\"))\n",
    "# print(embedding_test.shape)\n",
    "embedding_test = tf.expand_dims(embedding_test, axis=0)\n",
    "# print(embedding_test.shape)\n",
    "conv_1d = layers.Conv1D(\n",
    "    filters=32,\n",
    "    kernel_size=5,\n",
    "    activation=\"relu\",\n",
    "    padding=\"valid\"\n",
    ")\n",
    "conv_1d_output = conv_1d(embedding_test)\n",
    "max_pool = layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output)\n",
    "\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 - 1D Convolutional Network layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"input_layer\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(\n",
    "    filters=64,\n",
    "    kernel_size=5,\n",
    "    strides=1,\n",
    "    activation=\"relu\",\n",
    "    padding=\"valid\"\n",
    ")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_conv1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_conv1d\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 11, 64)            41024     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,321,089\n",
      "Trainable params: 1,321,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Checking model details\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_5.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_5_conv1d/20230116-103237\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 14ms/step - loss: 0.1226 - accuracy: 0.9622 - val_loss: 0.9535 - val_accuracy: 0.7717\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.0765 - accuracy: 0.9717 - val_loss: 1.0210 - val_accuracy: 0.7664\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.0610 - accuracy: 0.9766 - val_loss: 1.1577 - val_accuracy: 0.7651\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.0554 - accuracy: 0.9791 - val_loss: 1.1890 - val_accuracy: 0.7585\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.0504 - accuracy: 0.9788 - val_loss: 1.2463 - val_accuracy: 0.7572\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "history_5 = model_5.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_5_conv1d\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.89870685e-01],\n",
       "       [8.83843005e-01],\n",
       "       [9.99887943e-01],\n",
       "       [1.09054685e-01],\n",
       "       [1.31184677e-07],\n",
       "       [9.96619225e-01],\n",
       "       [9.56514895e-01],\n",
       "       [9.99980628e-01],\n",
       "       [9.99997675e-01],\n",
       "       [8.88097584e-01]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting probs into label format\n",
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.7217847769029,\n",
       " 'precision': 0.7576149234046069,\n",
       " 'recall': 0.7572178477690289,\n",
       " 'f1': 0.7555947144839635}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating model's predictions\n",
    "model_5_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_5_preds\n",
    ")\n",
    "model_5_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6 - TensorflowHub pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01602832  0.01068848  0.02425467 -0.01405769  0.01434426  0.08292625\n",
      "  0.01963372  0.06160142 -0.00352702 -0.01216412  0.00978647 -0.01248495\n",
      "  0.01232345  0.09748451  0.06141113 -0.03728353  0.01860884 -0.04669856\n",
      "  0.00413913 -0.06363907 -0.02469898  0.02713691  0.02284444 -0.00210026\n",
      " -0.00630592 -0.03964961  0.02220404  0.00115077 -0.03132177  0.00119527\n",
      " -0.0401255   0.04561892 -0.01530598 -0.00175918  0.02173131 -0.08450424\n",
      "  0.03340026  0.04604554 -0.0248025  -0.08681665  0.00702694 -0.00770478\n",
      " -0.01434539  0.07814164 -0.10676058 -0.05152997 -0.00858155 -0.03232232\n",
      " -0.03871097  0.02581467], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Reusing USE (Universal Sentence Encoder) model\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embed_samples = embed([\n",
    "    sample_sentence,\n",
    "    \"When you apple USE to text, it converts it into numbers\"\n",
    "])\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sampling shape\n",
    "embed_samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "# Creating layer with USE pretrained model\n",
    "sentence_encoder_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "    input_shape=[],\n",
    "    dtype=tf.string,\n",
    "    trainable=False,\n",
    "    name=\"USE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a new model with pretrained layer\n",
    "model_6 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    # layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "], name=\"model_6_use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_6.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_6_use/20230116-103312\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 17ms/step - loss: 0.6493 - accuracy: 0.7306 - val_loss: 0.6154 - val_accuracy: 0.7651\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.5820 - accuracy: 0.7840 - val_loss: 0.5660 - val_accuracy: 0.7743\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.5392 - accuracy: 0.7904 - val_loss: 0.5338 - val_accuracy: 0.7795\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.5106 - accuracy: 0.7961 - val_loss: 0.5127 - val_accuracy: 0.7848\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.4905 - accuracy: 0.7990 - val_loss: 0.4985 - val_accuracy: 0.7861\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "history_6 = model_6.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_6_use\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_use\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,798,337\n",
      "Trainable params: 513\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Checking the model details\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.37535816],\n",
       "       [0.68599105],\n",
       "       [0.858224  ],\n",
       "       [0.33574876],\n",
       "       [0.64923936],\n",
       "       [0.72946644],\n",
       "       [0.81911504],\n",
       "       [0.83147806],\n",
       "       [0.75724036],\n",
       "       [0.20284742]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "model_6_pred_probs = model_6.predict(val_sentences)\n",
    "model_6_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting prediction probs into label format\n",
    "model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n",
    "model_6_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.60892388451444,\n",
       " 'precision': 0.7863431959164349,\n",
       " 'recall': 0.7860892388451444,\n",
       " 'f1': 0.7850582651599072}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating model's performance metrics\n",
    "model_6_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_6_preds\n",
    ")\n",
    "model_6_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7 - Pretrained USE with 10% of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(761, 761)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## THIS IS WRONG WAY TO SPLIT, because of potential data leak,\n",
    "##  i.e. some data may end up in validation seubset\n",
    "\n",
    "# Creating subset of 10% training data\n",
    "train_data_10p = train_df_shuffled[[\"text\", \"target\"]].sample(frac=0.1, random_state=42)\n",
    "train_sentences_10p = train_data_10p[\"text\"].to_list()\n",
    "train_labels_10p = train_data_10p[\"target\"].to_list()\n",
    "len(train_sentences_10p), len(train_labels_10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685, 685)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MAKING A BETTER SPLIT\n",
    "\n",
    "# Creating 10% subset\n",
    "train_10p_split = int(0.1 * len(train_sentences))\n",
    "train_sentences_10p = train_sentences[:train_10p_split]\n",
    "train_labels_10p = train_labels[:train_10p_split]\n",
    "len(train_sentences_10p), len(train_labels_10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the length of original dataset\n",
    "len(train_df_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    413\n",
       "1    348\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of targets - 10%\n",
    "train_data_10p[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of targets - Full set\n",
    "train_df_shuffled[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_use\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,798,337\n",
      "Trainable params: 513\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cloning model 6\n",
    "model_7 = tf.keras.models.clone_model(model_6)\n",
    "\n",
    "# Compiling the model\n",
    "model_7.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Checking the model details\n",
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_7_use/20230116-184029\n",
      "Epoch 1/5\n",
      "22/22 [==============================] - 8s 94ms/step - loss: 0.6909 - accuracy: 0.5285 - val_loss: 0.6875 - val_accuracy: 0.5761\n",
      "Epoch 2/5\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.6776 - accuracy: 0.6993 - val_loss: 0.6785 - val_accuracy: 0.6601\n",
      "Epoch 3/5\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 0.6655 - accuracy: 0.7547 - val_loss: 0.6696 - val_accuracy: 0.6995\n",
      "Epoch 4/5\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.6544 - accuracy: 0.7723 - val_loss: 0.6611 - val_accuracy: 0.7008\n",
      "Epoch 5/5\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.6435 - accuracy: 0.7854 - val_loss: 0.6530 - val_accuracy: 0.7192\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "history_7 = model_7.fit(\n",
    "    train_sentences_10p,\n",
    "    train_labels_10p,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_7_use\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.46963853],\n",
       "       [0.5124494 ],\n",
       "       [0.57981694],\n",
       "       [0.47460523],\n",
       "       [0.50114006],\n",
       "       [0.51902497],\n",
       "       [0.54269725],\n",
       "       [0.49833354],\n",
       "       [0.52545536],\n",
       "       [0.45071054]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "model_7_pred_probs = model_7.predict(val_sentences)\n",
    "model_7_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 0., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting pred probs to label format\n",
    "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
    "model_7_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 71.91601049868767,\n",
       " 'precision': 0.730288919910709,\n",
       " 'recall': 0.7191601049868767,\n",
       " 'f1': 0.7101609145220378}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating model predictions\n",
    "model_7_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_7_preds\n",
    ")\n",
    "model_7_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8230a386d8e0083990873cddb8ebb5b6213275a10339230a8504f0ef8ce7f888"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
